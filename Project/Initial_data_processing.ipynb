{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e5f4a0b-6a09-46df-8abe-251d0ccd6350",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "657d6842-1543-4675-b4cf-5c234aafda17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import gzip\n",
    "import pickle\n",
    "import zipfile\n",
    "import requests\n",
    "from requests.exceptions import RequestException\n",
    "from typing import Iterable, Any, List, Dict, Optional\n",
    "from pathlib import Path\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26843805-6bab-4ed4-be08-4723d09e0f96",
   "metadata": {},
   "source": [
    "#### Download multivariate time-series data from GitHub\n",
    "\n",
    "GitHub repo: https://github.com/laiguokun/multivariate-time-series-data/tree/master\n",
    "\n",
    "Information about datasets: \n",
    "* Electricity consumption: The raw dataset is in https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014 (electricity consumption in kWh recorded every 15 minutes from 2011 to 2014). After processing, the dataset records the hourly electricity consumption of 321 clients from 2012 to 2014.\n",
    "* Traffic Usage: The raw data is in http://pems.dot.ca.gov. After processing, the dataset is a collection of 48 months (2015-2016) of hourly data from the California Department of Transportation. The data describes the road occupancy rates (between 0 and 1) measured by different sensors on San Francisco Bay area freeways.\n",
    "* Solar Energy: The raw data is in http://www.nrel.gov/grid/solar-power-data.html. After processing, the dataset contains the solar power production records in the year of 2006, which is sampled every 10 minutes from 137 PV plants in Alabama State. \n",
    "* Exchange Rate: The collection of the daily exchange rates of eight foreign countries including Australia, British, Canada, Switzerland, China, Japan, New Zealand and Singapore ranging from 1990 to 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24c2b2dd-0fe3-4316-985a-37d66eff6a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GithubTxtGzDownloader:\n",
    "    \"\"\"\n",
    "    Download all `.txt.gz` files from a GitHub repository (any subdirectory)\n",
    "    and save them under the same subfolder structure in the current working dir.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        repo_owner: str,\n",
    "        repo_name: str,\n",
    "        branch: Optional[str] = None,  # if None, try 'main' then 'master'\n",
    "        save_root: Path | str = \".\",\n",
    "    ):\n",
    "        self.repo_owner = repo_owner\n",
    "        self.repo_name = repo_name\n",
    "        self.branch = branch\n",
    "        self.save_root = Path(save_root)\n",
    "\n",
    "    def _zip_url(self, branch: str) -> str:\n",
    "        return f\"https://codeload.github.com/{self.repo_owner}/{self.repo_name}/zip/refs/heads/{branch}\"\n",
    "\n",
    "    def _get_repo_zip_bytes(self) -> bytes:\n",
    "        branches_to_try = [self.branch] if self.branch else [\"main\", \"master\"]\n",
    "        if self.branch and self.branch not in branches_to_try:\n",
    "            branches_to_try.insert(0, self.branch)\n",
    "\n",
    "        last_status = None\n",
    "        for br in branches_to_try:\n",
    "            url = self._zip_url(br)\n",
    "            resp = requests.get(url, timeout=60)\n",
    "            last_status = resp.status_code\n",
    "            if resp.status_code == 200:\n",
    "                self.branch = br  # remember the branch that worked\n",
    "                return resp.content\n",
    "\n",
    "        raise RuntimeError(\n",
    "            f\"Failed to download repository zip. \"\n",
    "            f\"Tried branches {branches_to_try}, last HTTP status: {last_status}\"\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _strip_top_level_dir(zip_path: str) -> str:\n",
    "        \"\"\"\n",
    "        Remove the first path component added by GitHub zip (e.g. 'repo-branch/').\n",
    "        'repo-branch/sub/dir/file.txt.gz' -> 'sub/dir/file.txt.gz'\n",
    "        \"\"\"\n",
    "        parts = zip_path.split(\"/\", maxsplit=1)\n",
    "        return parts[1] if len(parts) > 1 else \"\"\n",
    "\n",
    "    def download(self) -> int:\n",
    "        \"\"\"\n",
    "        Download and save all .txt.gz files. Returns the number of files saved.\n",
    "        \"\"\"\n",
    "        zip_bytes = self._get_repo_zip_bytes()\n",
    "        saved = 0\n",
    "\n",
    "        with zipfile.ZipFile(io.BytesIO(zip_bytes)) as zf:\n",
    "            for info in zf.infolist():\n",
    "                # Skip directories\n",
    "                if info.is_dir():\n",
    "                    continue\n",
    "\n",
    "                # Normalize path to drop top-level folder\n",
    "                rel_path = self._strip_top_level_dir(info.filename)\n",
    "                if not rel_path:\n",
    "                    continue\n",
    "\n",
    "                # Only keep .txt.gz files\n",
    "                if not rel_path.lower().endswith(\".txt.gz\"):\n",
    "                    continue\n",
    "\n",
    "                # Build local path (mirror subdirectories)\n",
    "                local_path = self.save_root / rel_path\n",
    "                local_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                # Copy raw bytes (do NOT decode)\n",
    "                with zf.open(info, \"r\") as src, open(local_path, \"wb\") as dst:\n",
    "                    shutil.copyfileobj(src, dst)\n",
    "\n",
    "                saved += 1\n",
    "\n",
    "        return saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3abfbed1-ad47-4de6-ba87-d8da252048df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 4 .txt.gz files.\n"
     ]
    }
   ],
   "source": [
    "# Download .txt.gz from the laiguokun/multivariate-time-series-data GitHub repo\n",
    "downloader = GithubTxtGzDownloader(repo_owner=\"laiguokun\",\n",
    "                                   repo_name=\"multivariate-time-series-data\",\n",
    "                                   branch=\"master\", save_root=\".\", # save under current working directory\n",
    "                                  )\n",
    "count = downloader.download()\n",
    "print(f\"Saved {count} .txt.gz files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9f832e-fd3b-4164-94da-4f06ea8d5906",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc0bb7f4-9fa7-49ca-96e0-abe00158a054",
   "metadata": {},
   "source": [
    "#### Download textual data from arXiv\n",
    "\n",
    "Paper: \"Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks\" by Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, Xiaojun Chang, Chengqi Zhang. May 2020. Available at arXiv:2005.11650 (url: https://arxiv.org/abs/2005.11650)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ac76ba6-4d65-445e-a1d5-2d73a9a10ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_arxiv_id(s: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Extract an arXiv ID from a URL or return the ID if already given.\n",
    "    Supports new-style IDs (e.g., 2005.11650 or 2005.11650v2).\n",
    "    Also handles arxiv.org/abs/<id> and arxiv.org/pdf/<id>.pdf.\n",
    "    \"\"\"\n",
    "    s = (s or \"\").strip()\n",
    "\n",
    "    # From URL: /abs/<id> or /pdf/<id>.pdf\n",
    "    m = re.search(r\"arxiv\\.org/(?:pdf|abs)/([^\\s/#?]+)\", s, flags=re.IGNORECASE)\n",
    "    if m:\n",
    "        raw = m.group(1)\n",
    "        if raw.lower().endswith(\".pdf\"):\n",
    "            raw = raw[:-4]\n",
    "        return raw\n",
    "\n",
    "    # New-style ID directly\n",
    "    if re.fullmatch(r\"\\d{4}\\.\\d{4,5}(?:v\\d+)?\", s):\n",
    "        return s\n",
    "\n",
    "    # (Optional) very light old-style support like cs/0704xxx(vN)\n",
    "    if re.fullmatch(r\"[a-z\\-]+(?:\\.[A-Z]{2})?/\\d{7}(?:v\\d+)?\", s, flags=re.IGNORECASE):\n",
    "        return s\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def fetch_arxiv_textual_content(arxiv_url_or_id: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Fetch readable text for an arXiv paper by resolving to ar5iv (HTML) and\n",
    "    passing that through Jina Reader. Falls back to the arXiv ABS page.\n",
    "\n",
    "    Args:\n",
    "        arxiv_url_or_id: e.g. 'https://arxiv.org/pdf/2005.11650',\n",
    "                         'https://arxiv.org/abs/2005.11650v2', or '2005.11650'\n",
    "\n",
    "    Returns:\n",
    "        Plain text (str) or None on failure.\n",
    "    \"\"\"\n",
    "    if not arxiv_url_or_id or not isinstance(arxiv_url_or_id, str):\n",
    "        raise ValueError(\"The 'arxiv_url_or_id' parameter must be a non-empty string.\")\n",
    "\n",
    "    arxiv_id = _extract_arxiv_id(arxiv_url_or_id)\n",
    "    if not arxiv_id:\n",
    "        raise ValueError(\"Could not extract a valid arXiv ID from the input.\")\n",
    "\n",
    "    jina_base = \"https://r.jina.ai/\"\n",
    "    ar5iv_url = f\"https://ar5iv.org/html/{arxiv_id}\"\n",
    "    abs_url = f\"https://arxiv.org/abs/{arxiv_id}\"\n",
    "\n",
    "    # Try ar5iv first (best structured text), then fallback to ABS page\n",
    "    for target in (ar5iv_url, abs_url):\n",
    "        try:\n",
    "            resp = requests.get(jina_base + target.lstrip(\"/\"), timeout=20)\n",
    "            resp.raise_for_status()\n",
    "            return resp.content.decode(\"utf-8\", errors=\"replace\")\n",
    "        except RequestException as e:\n",
    "            print(f\"Error fetching via Jina '{target}': {e}\")\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"Error decoding response for '{target}'.\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea40cd26-4b41-411a-9b2a-b93ecb16e595",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = fetch_arxiv_textual_content(\"https://arxiv.org/pdf/2005.11650\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8605a96-048b-45ca-a02c-c4bad7c27773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks\n",
      "\n",
      "URL Source: https://ar5iv.org/html/2005.11650\n",
      "\n",
      "Markdown Content:\n",
      "(2020)\n",
      "\n",
      "###### Abstract.\n",
      "\n",
      "Modeling multivariate time series has long been a subject that has attracted researchers from a diverse range of fields including economics, finance, and traffic. A basic assumption behind multivariate time series forecasting is that its variables depend on one another but, upon looking closely, it’s fair to say that existing methods fail to fully exploit latent spatial dependencies between pairs of variables. In recent years, meanwhile, graph neural networks (GNNs) have shown high capability in handling relational dependencies. GNNs require well-defined graph structures for information propagation which means they cannot be applied directly for multivariate time series where the dependencies are not known in advance. In this paper, we propose a general graph neural network framework designed specifically for multivariate time series data. Our approach automatically extracts the uni-directed relations among variables through a graph learning module, into which external knowledge like variable attributes can be easily integrated. A novel mix-hop propagation layer and a dilated inception layer are further proposed to capture the spatial and temporal dependencies within the time series. The graph learning, graph convolution, and temporal convolution modules are jointly learned in an end-to-end framework. Experimental results show that our proposed model outperforms the state-of-the-art baseline methods on 3 of 4 benchmark datasets and achieves on-par performance with other approaches on two traffic datasets which provide extra structural information.\n",
      "\n",
      "Graph neural networks, graph structure learning, multivariate time series forecasting, spatial-temporal graphs\n",
      "\n",
      "††copyright: acmcopyright††journalyear: 2020††copyright: acmcopyright††conference: 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining; August 23–27, 2020; Virtual Event, USA††booktitle: 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ’20), August 23–27, 2020, Virtual Event, USA††price: 15.00††doi: 10.1145/XXXXXX.XXXXXX††isbn: 978-1-4503-7998-4/20/08††ccs: Computing methodologies Neural networks††ccs: Computing methodologies Artificial intelligence\n",
      "1. Introduction\n",
      "---------------\n",
      "\n",
      "Modern societies have benefited from a wide range of sensors to record changes in temperature, price, traffic speed, electricity usage, and many other forms of data. Recorded time series from different sensors can form multivariate time series data and can be interlinked. For example, the rise in daily temperature may cause an increase in electricity usage. To capture systematic trends over a group of dynamically changing variables, the problem of multivariate time series forecasting has been studied for at least sixty years. It has seen tremendous applications in the domains of economics, finance, bioinformatics, and traffic.\n",
      "\n",
      "Multivariate time series forecasting methods inherently assume interdependencies among variables. In other words, each variable depends not only on its historical values but also on other variables. However, existing methods do not exploit latent interdependencies among variables efficiently and effectively. Statistical methods, such as vector auto-regressive model (VAR) and Gaussian process model (GP), assume a linear dependency among variables. The model complexity of statistical methods grows quadratically with the number of variables. They face the problem of overfitting with a large number of variables. Recently developed deep-learning-based methods, including LSTNet (Lai et al., [2018](https://ar5iv.org/html/2005.11650#bib.bib13)) and TPA-LSTM (Shih et al., [2019](https://ar5iv.org/html/2005.11650#bib.bib20)), are powerful to capture non-linear patterns. LSTNet encodes short-term local information into low dimensional vectors using 1D convolutional neural networks and decodes the vectors through a recurrent neural network. TPA-LSTM processes the inputs by a recurrent neural network and employs a convolutional neural network to calculate the attention score across multiple steps. LSTNet and TPA-LSTM do not model the pair-wise dependencies among variables explicitly, which weakens model interpretability.\n",
      "\n",
      "Graphs are a special form of data which describes the relationships between different entities. Recently, graph neural networks have achieved great success in handling graph data due to their permutation-invariance, local connectivity, and compositionality. By propagating information through structures, graph neural networks allow each node in a graph to be aware of its neighborhood context. Multivariate time series forecasting can be viewed naturally from a graph perspective. Variables from multivariate time series can be considered as nodes in a graph, and they are interlinked through their hidden dependency relationships. It follows that modeling multivariate time series data using graph neural networks can be a promising way to preserve their temporal trajectory while exploiting the interdependency among time series.\n",
      "\n",
      "The most suitable type of graph neural networks for multivariate time series is spatial-temporal graph neural networks. Spatial-temporal graph neural networks take multivariate time series and an external graph structure as inputs, and they aim to predict future values or labels of multivariate time series. Spatial-temporal graph neural networks have achieved significant improvements compared to methods that do not utilize structural information. However, these approaches still fall short for modeling multivariate time series due to the following challenges:\n",
      "\n",
      "*   •\n",
      "Challenge 1: Unknown Graph Structure. Existing GNN approaches rely heavily on a pre-defined graph structure in order to perform time series forecasting. In most cases, multivariate time series does not have an explicit graph structure. The relationships among variables has to be discovered from data rather than being provided as ground truth knowledge.\n",
      "\n",
      "*   •\n",
      "Challenge 2: Graph Learning & GNN Learning.  Even though a graph structure is available, most GNN approaches focus only on message passing (GNN Learning) and overlook the fact that the graph structure is not optimal and should be updated during training. The question then is how to simultaneously learn the graph structure and the GNN for time series in an end-to-end framework.\n",
      "\n",
      "In this paper, we propose a novel approach to overcome these challenges. As demonstrated by Figure [1](https://ar5iv.org/html/2005.11650#S1.F1 \"Figure 1 ‣ 1. Introduction ‣ Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks\"), our framework consists of three core components - the graph learning layer, the graph convolution module, and the temporal convolution module. For Challenge 1, we propose a novel graph learning layer, which extracts a sparse graph adjacency matrix adaptively based on data. Furthermore, we develop a graph convolution module to address the spatial dependencies among variables, given the adjacency matrix computed by the graph learning layer. This is designed specifically for directed graphs and avoids the over-smoothing problem that frequently occurs in graph convolutional networks. Finally, we propose a temporal convolution module to capture temporal patterns by modified 1D convolutions. It can both discover temporal patterns with multiple frequencies and process very long sequences.\n",
      "\n",
      "As all parameters are learnable through gradient descent, the proposed framework is able to model multivariate time series data and learn the internal graph structure simultaneously in an end-to-end manner (for Challenge 2). To reduce the difficulty of solving a highly non-convex optimization problem and to reduce memory occupation in processing large graphs, we propose a learning algorithm that uses a curriculum learning strategy to find a better local optimum and splits multivariate time series into subgroups during training. The advantages here are that our proposed framework is generally applicable to both small and large graphs, short and long time series, with and without externally defined graph structures. In summary, our main contributions are as follows:\n",
      "\n",
      "*   •\n",
      "To the best of our knowledge, this is the first study on multivariate time series data generally from a graph-based perspective with graph neural networks.\n",
      "\n",
      "*   •\n",
      "We propose a novel graph learning module to learn hidden spatial dependencies among variables. Our method opens a new door for GNN models to handle data without explicit graph structure.\n",
      "\n",
      "*   •\n",
      "We present a joint framework for modeling multivariate time series data and learning graph structures. Our framework is more generic than any existing spatial-temporal graph neural network as it can handle multivariate time series with or without a pre-defined graph structure.\n",
      "\n",
      "*   •\n",
      "Experimental results show that our method outperforms the state-of-the-art methods on 3 of 4 benchmark datasets and achieves on-par performance with other GNNs on two traffic datasets which provide extra structural information.\n",
      "\n",
      "![Image 1: Refer to caption](https://ar5iv.labs.arxiv.org/html/2005.11650/assets/x1.png)\n",
      "\n",
      "Figure 1. A concept map of our proposed framework. \n",
      "\n",
      "2. Backgrounds\n",
      "--------------\n",
      "\n",
      "### 2.1. Multivariate Time Series Forecasting\n",
      "\n",
      "Time series forecasting has been studied for a long time. The majority of existing methods follow a statistical approach. The auto-regressive integrated moving average (ARIMA) (Box et al., [2015](https://ar5iv.org/html/2005.11650#bib.bib2)) generalizes a family of a linear model, including auto-regressive (AR), moving average (MA), and auto-regressive moving average (ARMA). The vector auto-regressive model (VAR) extends the AR model to capture the linear interdependencies among multiple time series. Similarly, the vector auto-regressive moving average model (VARMA) is proposed as a multivariate version of the ARMA model. Gaussian process (GP), as a Bayesian approach, models the distribution of a multivariate variable over functions. GP can be applied naturally to model multivariate time series data (Frigola, [2015](https://ar5iv.org/html/2005.11650#bib.bib6)). Although statistical models are widely used in time series forecasting due to their simplicity and interpretability, they make strong assumptions with respect to a stationary process and they do not scale well to multivariate time series data. Deep-learning-based approaches are free from stationary assumptions and they are effective methods to capture non-linearity. Lai et al. (Lai et al., [2018](https://ar5iv.org/html/2005.11650#bib.bib13)) and Shih et al. (Shih et al., [2019](https://ar5iv.org/html/2005.11650#bib.bib20)) are the first two deep-learning-based models designed for multivariate time series forecasting. They employ convolutional neural networks to capture local dependencies among variables and recurrent neural networks to preserve long-term temporal dependencies. Convolutional neural networks encapsulate interactions among variables into a global hidden state. Therefore, they cannot fully exploit latent dependencies between pairs of variables.\n",
      "\n",
      "### 2.2. Graph Neural Networks\n",
      "\n",
      "Graph neural networks have enjoyed great success in handling spatial dependencies among entities in a network. Graph neural networks assume that the state of a node depends on the states of its neighbors. To capture this type of spatial dependency, various kinds of graph neural networks have been developed through message passing (Gilmer et al., [2017](https://ar5iv.org/html/2005.11650#bib.bib8)), information propagation (Klicpera et al., [2019](https://ar5iv.org/html/2005.11650#bib.bib12)), and graph convolution (Kipf and Welling, [2017](https://ar5iv.org/html/2005.11650#bib.bib11)). Sharing similar roles, they essentially capture a node’s high-level representation by passing information from a node’s neighbors to the node itself. Most recently, we have seen the emergence of a type of graph neural networks known as spatial-temporal graph neural networks. This form of neural networks is proposed initially to solve the problem of traffic prediction (Li et al., [2018](https://ar5iv.org/html/2005.11650#bib.bib14); Yu et al., [2018](https://ar5iv.org/html/2005.11650#bib.bib24); Wu et al., [2019](https://ar5iv.org/html/2005.11650#bib.bib22); Zheng et al., [2020](https://ar5iv.org/html/2005.11650#bib.bib27); Chen et al., [2019a](https://ar5iv.org/html/2005.11650#bib.bib4)) and skeleton-based action recognition (Yan et al., [2018](https://ar5iv.org/html/2005.11650#bib.bib23); Shi et al., [2019](https://ar5iv.org/html/2005.11650#bib.bib19)). The inputs to spatial-temporal graph neural networks are multivariate time series with an external graph structure which describes the relationships among variables in multivariate time series. For spatial-temporal graph neural networks, spatial dependencies among nodes are captured by graph convolutions, while temporal dependencies among historical states are preserved by recurrent neural networks (Seo et al., [2018](https://ar5iv.org/html/2005.11650#bib.bib18); Li et al., [2018](https://ar5iv.org/html/2005.11650#bib.bib14)) or 1D convolutions (Yu et al., [2018](https://ar5iv.org/html/2005.11650#bib.bib24); Yan et al., [2018](https://ar5iv.org/html/2005.11650#bib.bib23)). Although existing spatial-temporal graph neural networks have achieved significant improvements compared to methods without using a graph structure, they are incapable of handling pure multivariate time series data effectively due to the absence of a pre-defined graph and lack of a general framework.\n",
      "\n",
      "3. Problem Formulation\n",
      "----------------------\n",
      "\n",
      "In this paper, we focus on the task of multivariate time series forecasting. Let 𝐳 t∈𝐑 N subscript 𝐳 𝑡 superscript 𝐑 𝑁\\mathbf{z}_{t}\\in\\mathbf{R}^{N} denote the value of a multivariate variable of dimension N 𝑁 N at time step t, where z t​[i]∈R subscript 𝑧 𝑡 delimited-[]𝑖 𝑅 z_{t}[i]\\in R denote the value of the i t​h superscript 𝑖 𝑡 ℎ i^{th} variable at time step t. Given a sequence of historical P 𝑃 P time steps of observations on a multivariate variable, 𝐗={𝐳 t 1,𝐳 t 2,⋯,𝐳 t P}𝐗 subscript 𝐳 subscript 𝑡 1 subscript 𝐳 subscript 𝑡 2⋯subscript 𝐳 subscript 𝑡 𝑃\\mathbf{X}=\\{\\mathbf{z}_{t_{1}},\\mathbf{z}_{t_{2}},\\cdots,\\mathbf{z}_{t_{P}}\\}, our goal is to predict the Q 𝑄 Q-step-away value of 𝐘={𝐳 t P+Q}𝐘 subscript 𝐳 subscript 𝑡 𝑃 𝑄\\mathbf{Y}=\\{\\mathbf{z}_{t_{P+Q}}\\}, or a sequence of future values 𝐘={𝐳 t P+1,𝐳 t P+2,⋯,𝐳 t P+Q}𝐘 subscript 𝐳 subscript 𝑡 𝑃 1 subscript 𝐳 subscript 𝑡 𝑃 2⋯subscript 𝐳 subscript 𝑡 𝑃 𝑄\\mathbf{Y}=\\{\\mathbf{z}_{t_{P+1}},\\mathbf{z}_{t_{P+2}},\\cdots,\\mathbf{z}_{t_{P+Q}}\\}. More generally, the input signals can be coupled with other auxiliary features such as time of the day, day of the week, and day of the season. Concatenating the input signals with auxiliary features, we assume the inputs instead are 𝒳={𝐒 t 1,𝐒 t 2,⋯,𝐒 t P}𝒳 subscript 𝐒 subscript 𝑡 1 subscript 𝐒 subscript 𝑡 2⋯subscript 𝐒 subscript 𝑡 𝑃\\mathbf{\\mathcal{X}}=\\{\\mathbf{S}_{t_{1}},\\mathbf{S}_{t_{2}},\\cdots,\\mathbf{S}_{t_{P}}\\} where 𝐒 t i∈𝐑 N×D subscript 𝐒 subscript 𝑡 𝑖 superscript 𝐑 𝑁 𝐷\\mathbf{S}_{t_{i}}\\in\\mathbf{R}^{N\\times D}, D 𝐷 D is the feature dimension, the first column of 𝐒 t i subscript 𝐒 subscript 𝑡 𝑖\\mathbf{S}_{t_{i}} equals to 𝐳 t i subscript 𝐳 subscript 𝑡 𝑖\\mathbf{z}_{t_{i}}, and the rest are auxiliary features. We aim to build a mapping f​(⋅)𝑓⋅f(\\cdot) from 𝒳 𝒳\\mathbf{\\mathcal{X}} to 𝐘 𝐘\\mathbf{Y} by minimizing the absolute loss with l​2 𝑙 2 l2 regularization.\n",
      "\n",
      "Graphs describe the relationships among entities in a network. We give a formal definition of graph-related concepts below.\n",
      "\n",
      "###### Definition 3.1 (Graph).\n",
      "\n",
      "A graph is formulated as G 𝐺 G = (V,E)𝑉 𝐸(V,E) where V 𝑉 V is the set of nodes, and E 𝐸 E is the set of edges. We use N 𝑁 N to denote the number of nodes in a graph.\n",
      "\n",
      "###### Definition 3.2 (Node Neighborhood).\n",
      "\n",
      "Let v∈V 𝑣 𝑉 v\\in V to denote a node and e=(v,u)∈E 𝑒 𝑣 𝑢 𝐸 e=(v,u)\\in E to denote an edge pointing from u 𝑢 u to v 𝑣 v. The neighborhood of a node v 𝑣 v is defined as N​(v)={u∈V|(v,u)∈E}𝑁 𝑣 conditional-set 𝑢 𝑉 𝑣 𝑢 𝐸 N(v)=\\{u\\in V|(v,u)\\in E\\}.\n",
      "\n",
      "###### Definition 3.3 (Adjacency Matrix).\n",
      "\n",
      "The adjacency matrix is a mathematical representation of a graph, denoted as 𝐀∈R N×N 𝐀 superscript 𝑅 𝑁 𝑁\\mathbf{A}\\in R^{N\\times N} with A i​j=c>0 subscript 𝐴 𝑖 𝑗 𝑐 0 A_{ij}=c>0 if (v i,v j)∈E subscript 𝑣 𝑖 subscript 𝑣 𝑗 𝐸(v_{i},v_{j})\\in E and A i​j=0 subscript 𝐴 𝑖 𝑗 0 A_{ij}=0 if (v i,v j)∉E subscript 𝑣 𝑖 subscript 𝑣 𝑗 𝐸(v_{i},v_{j})\\notin E.\n",
      "\n",
      "From a graph-based perspective, we consider variables in multivariate time series as nodes in graphs. We describe the relationships among nodes using the graph adjacency matrix. The graph adjacency matrix is not given by the multivariate time series data in most cases and will be learned by our model.\n",
      "\n",
      "4. Framework of MTGNN\n",
      "---------------------\n",
      "\n",
      "![Image 2: Refer to caption](https://ar5iv.labs.arxiv.org/html/2005.11650/assets/x2.png)\n",
      "\n",
      "Figure 2. The framework of MTGNN. A 1×1 1 1 1\\times 1 standard convolution first projects the inputs into a latent space. Afterward, temporal convolution modules and graph convolution modules are interleaved with each other to capture temporal and spatial dependencies respectively. The hyper-parameter, dilation factor d 𝑑 d, which controls the receptive field size of a temporal convolution module, is increased at an exponential rate of q 𝑞 q. The graph learning layer learns the hidden graph adjacency matrix, which is used by graph convolution modules. Residual connections and skip connections are added to the model to avoid the problem of gradient vanishing. The output module projects hidden features to the desired dimension to get the final results.\n",
      "\n",
      "![Image 3: Refer to caption](https://ar5iv.labs.arxiv.org/html/2005.11650/assets/x3.png)\n",
      "\n",
      "Figure 3. A demonstration of how a temporal convolution module and a graph convolution module collaborate with each other. A temporal convolution module filters the inputs by sliding a 1D window over the time and node axes, as denoted by the red. A graph convolution module filters the inputs at each step, denoted by the blue.\n",
      "\n",
      "### 4.1. Model Architecture\n",
      "\n",
      "We first elaborate on the general framework of our model. As illustrated in Figure [2](https://ar5iv.org/html/2005.11650#S4.F2 \"Figure 2 ‣ 4. Framework of MTGNN ‣ Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks\"), MTGNN on the highest level consists of a graph learning layer, m 𝑚 m graph convolution modules, m 𝑚 m temporal convolution modules, and an output module. To discover hidden associations among nodes, a graph learning layer computes a graph adjacency matrix, which is later used as an input to all graph convolution modules. Graph convolution modules are interleaved with temporal convolution modules to capture spatial and temporal dependencies respectively. Figure [3](https://ar5iv.org/html/2005.11650#S4.F3 \"Figure 3 ‣ 4. Framework of MTGNN ‣ Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks\") gives a demonstration of how a temporal convolution module and a graph convolution module collaborate with each other. To avoid the problem of gradient vanishing, residual connections are added from the inputs of a temporal convolution module to the outputs of a graph convolution module. Skip connections are added after each temporal convolution module. To get the final outputs, the output module projects the hidden features to the desired output dimension. In more detail, the core components of our model are illustrated in the following:\n",
      "\n",
      "### 4.2. Graph Learning Layer\n",
      "\n",
      "The graph learning layer learns a graph adjacency matrix adaptively to capture the hidden relationships among time series data. To construct a graph, existing studies measure the similarity between pairs of nodes by a distance metric, such as dot product and Euclidean distance (Li et al., [2018](https://ar5iv.org/html/2005.11650#bib.bib14)). This leads inevitably to the problem of high time and space complexity with O​(N 2)𝑂 superscript 𝑁 2 O(N^{2}). It means the computation and memory cost grows quadratically with the increase of graph size. This restricts the model’s capability of handling larger graphs. To address this limitation, we adopt a sampling approach, which only calculates pair-wise relationships among a subset of nodes. This cuts off the bottleneck of computation and memory in each minibatch. More details will be provided in Section [4.6](https://ar5iv.org/html/2005.11650#S4.SS6 \"4.6. Proposed Learning Algorithm ‣ 4. Framework of MTGNN ‣ Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks\").\n",
      "\n",
      "Another problem is that existing distance metrics are often symmetric or bi-directional. In multivariate time series forecasting, we expect that the change of a node’s condition causes the change of another node’s condition such as traffic flow. Therefore the learned relation is supposed to be uni-directional. Our proposed graph learning layer is specifically designed to extract uni-directional relationships, illustrated as follows:\n",
      "\n",
      "(1)𝐌 1 subscript 𝐌 1\\displaystyle\\mathbf{M}_{1}=t​a​n​h​(α​𝐄 1​𝚯 1)absent 𝑡 𝑎 𝑛 ℎ 𝛼 subscript 𝐄 1 subscript 𝚯 1\\displaystyle=tanh(\\alpha\\mathbf{E}_{1}\\mathbf{\\Theta}_{1})\n",
      "(2)𝐌 2 subscript 𝐌 2\\displaystyle\\mathbf{M}_{2}=t​a​n​h​(α​𝐄 2​𝚯 2)absent 𝑡 𝑎 𝑛 ℎ 𝛼 subscript 𝐄 2 subscript 𝚯 2\\displaystyle=tanh(\\alpha\\mathbf{E}_{2}\\mathbf{\\Theta}_{2})\n",
      "(3)𝐀 𝐀\\displaystyle\\mathbf{A}=R​e​L​U​(t​a​n​h​(α​(𝐌 1​𝐌 2 T−𝐌 2​𝐌 1 T)))absent 𝑅 𝑒 𝐿 𝑈 𝑡 𝑎 𝑛 ℎ 𝛼 subscript 𝐌 1 superscript subscript 𝐌 2 𝑇 subscript 𝐌 2 superscript subscript 𝐌 1 𝑇\\displaystyle=ReLU(tanh(\\alpha(\\mathbf{M}_{1}\\mathbf{M}_{2}^{T}-\\mathbf{M}_{2}\\mathbf{M}_{1}^{T})))\n",
      "(4)f​o​r 𝑓 𝑜 𝑟\\displaystyle for i=1,2,⋯,N 𝑖 1 2⋯𝑁\\displaystyle\\;i=1,2,\\cdots,N\n",
      "(5)𝐢𝐝𝐱=a​r​g​t​o​p​k​(𝐀​[i,:])𝐢𝐝𝐱 𝑎 𝑟 𝑔 𝑡 𝑜 𝑝 𝑘 𝐀 𝑖:\\displaystyle\\mathbf{idx}=argtopk(\\mathbf{A}[i,:])\n",
      "(6)𝐀​[i,−𝐢𝐝𝐱]=0,𝐀 𝑖 𝐢𝐝𝐱 0\\displaystyle\\mathbf{A}[i,-\\mathbf{idx}]=0,\n",
      "\n",
      "where 𝐄 1,𝐄 2 subscript 𝐄 1 subscript 𝐄 2\\mathbf{E}_{1},\\mathbf{E}_{2} represents randomly initialized node embeddings, which are learnable during training, Θ 1,Θ 2 subscript Θ 1 subscript Θ 2\\Theta_{1},\\Theta_{2} are model parameters, α 𝛼\\alpha is a hyper-parameter for controlling the saturation rate of the activation function, and a​r​g​t​o​p​k​(⋅)𝑎 𝑟 𝑔 𝑡 𝑜 𝑝 𝑘⋅argtopk(\\cdot) returns the index of the top-k largest values of a vector. The asymmetric property of our proposed graph adjacency matrix is achieved by Equation [3](https://ar5iv.org/html/2005.11650#S4.E3 \"In 4.2. Graph Learning Layer ‣ 4. Framework of MTGNN ‣ Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks\"). The subtraction term and the ReLU activation function regularize the adjacency matrix so that if A v​u subscript 𝐴 𝑣 𝑢 A_{vu} is positive, its diagonal counterpart A u​v subscript 𝐴 𝑢 𝑣 A_{uv} will be zero. Equation [5](https://ar5iv.org/html/2005.11650#S4.E5 \"In 4.2. Graph Learning Layer ‣ 4. Framework of MTGNN ‣ Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks\")-[6](https://ar5iv.org/html/2005.11650#S4.E6 \"In 4.2. Graph Learning Layer ‣ 4. Framework of MTGNN ‣ Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks\") is a strategy to make the adjacency matrix sparse while reducing the computation cost of the following graph convolution. For each node, we select its top-k closest nodes as its neighbors. While retaining the weights for connected nodes, we set the weights of non-connected nodes as zero.\n",
      "\n",
      "Incorporate External Data. The inputs to the graph learning layer are not limited to node embeddings. In case that external knowledge about the attributes of each node is given, we can also set 𝐄 1=𝐄 2=𝐙 subscript 𝐄 1 subscript 𝐄 2 𝐙\\mathbf{E}_{1}=\\mathbf{E}_{2}=\\mathbf{Z}, where 𝐙 𝐙\\mathbf{Z} is a static node feature matrix. Some works have considered capturing dynamic spatial dependencies (Guo et al., [2019](https://ar5iv.org/html/2005.11650#bib.bib9); Shi et al., [2019](https://ar5iv.org/html/2005.11650#bib.bib19)). In other words, they dynamically adjust the weight of two connected nodes based on temporal inputs. However, assuming dynamic spatial dependencies makes the model extremely hard to converge when we need to learn the graph structure at the same time. The advantage of our approach is that we can learn stable and interpretable node relationships over the period of the training dataset. Once the model is trained in an on-line learning version, our graph adjacency matrix is also adaptable to change as new training data updates the model parameters.\n",
      "\n",
      "### 4.3. Graph Convolution Module\n",
      "\n",
      "The graph convolution module aims to fuse a node’s information with its neighbors’ information to handle spatial dependencies in a graph. The graph convolution module consists of two mix-hop propagation layers to process inflow and outflow information passed through each node separately. The net inflow information is obtained by adding the outputs of the two mix-hop propagation layers. Figure [4](https://ar5iv.org/html/2005.11650#S4.F4 \"Figure 4 ‣ 4.3. Graph Convolution Module ‣ 4. Framework of MTGNN ‣ Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks\") shows the architecture of the graph convolution module and the mix-hop propagation layer.\n",
      "\n",
      "![Image 4: Refer to caption](https://ar5iv.labs.arxiv.org/html/2005.11650/assets/x4.png)\n",
      "\n",
      "(a)GC module\n",
      "\n",
      "![Image 5: Refer to caption](https://ar5iv.labs.arxiv.org/html/2005.11650/assets/x5.png)\n",
      "\n",
      "(b)Mix-hop propagation layer\n",
      "\n",
      "Figure 4. Graph convolution and mix-hop propagation layer.\n",
      "\n",
      "##### Mix-hop Propagation Layer.\n",
      "\n",
      "Given a graph adjacency matrix, we propose the mix-hop propagation layer to handle information flow over spatially dependent nodes. The proposed mix-hop propagation layer consists of two steps - the information propagation step and the information selection step. We first give the mathematical form of these two steps and then illustrate our motivations. The information propagation step is defined as follows:\n",
      "\n",
      "(7)𝐇(k)=β​𝐇 i​n+(1−β)​𝐀~​𝐇(k−1),superscript 𝐇 𝑘 𝛽 subscript 𝐇 𝑖 𝑛 1 𝛽~𝐀 superscript 𝐇 𝑘 1\\mathbf{H}^{(k)}=\\beta\\mathbf{H}_{in}+(1-\\beta)\\tilde{\\mathbf{A}}\\mathbf{H}^{(k-1)},\n",
      "\n",
      "where β 𝛽\\beta is a hyper parameter, which controls the ratio of retaining the root node’s original states. The information selection step is defined as follows\n",
      "\n",
      "(8)𝐇 o​u​t=∑i=0 K 𝐇(k)​𝐖(k),subscript 𝐇 𝑜 𝑢 𝑡 superscript subscript 𝑖 0 𝐾 superscript 𝐇 𝑘 superscript 𝐖 𝑘\\mathbf{H}_{out}=\\sum_{i=0}^{K}\\mathbf{H}^{(k)}\\mathbf{W}^{(k)},\n",
      "\n",
      "where K 𝐾 K is the depth of propagation, 𝐇 i​n subscript 𝐇 𝑖 𝑛\\mathbf{H}_{in} represents the input hidden states outputted by the previous layer, 𝐇 o​u​t subscript 𝐇 𝑜 𝑢 𝑡\\mathbf{H}_{out} represents the output hidden states of the current layer, 𝐇(0)=𝐇 i​n superscript 𝐇 0 subscript 𝐇 𝑖 𝑛\\mathbf{H}^{(0)}=\\mathbf{H}_{in}, 𝐀~=𝐃~−1​(𝐀+𝐈)~𝐀 superscript~𝐃 1 𝐀 𝐈\\tilde{\\mathbf{A}}=\\tilde{\\mathbf{D}}^{-1}(\\mathbf{A}+\\mathbf{I}), and 𝐃~i​i=1+∑j 𝐀 i​j subscript~𝐃 𝑖 𝑖 1 subscript 𝑗 subscript 𝐀 𝑖 𝑗\\tilde{\\mathbf{D}}_{ii}=1+\\sum_{j}\\mathbf{A}_{ij}. In Figure [4(b)](https://ar5iv.org/html/2005.11650#S4.F4.sf2 \"In Figure 4 ‣ 4.3. Graph Convolution Module ‣ 4. Framework of MTGNN ‣ Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks\"), we demonstrate the information propagation step and information selection step in the proposed mix-hop propagation layer. It first propagates information horizontally and selects information vertically.\n",
      "\n",
      "The information propagation step propagates node information along with the given graph structure recursively. A severe limitation of graph convolutional networks is that node hidden states converge to a single point as the number of graph convolution layers goes to infinity. This is because the graph convolutional network with many layers reaches the random walk’s limit distribution regardless of the initial node states. To address this problem, motivated by Klicpera et al. (Klicpera et al., [2019](https://ar5iv.org/html/2005.11650#bib.bib12)), we retain a proportion of nodes’ original states during the propagation process so that the propagated node states can both preserve locality and explore a deep neighborhood. However, if we only apply Equation [7](https://ar5iv.org/html/2005.11650#S4.E7 \"In Mix-hop Propagation Layer. ‣ 4.3. Graph Convolution Module ‣ 4. Framework of MTGNN ‣ Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks\"), some node information will be lost. Under the extreme circumstance that no spatial dependencies exist, aggregating neighborhood information simply adds useless noises to each node. Therefore, the information selection step is introduced to filter out important information produced at each hop. According to Equation [8](https://ar5iv.org/html/2005.11650#S4.E8 \"In Mix-hop Propagation Layer. ‣ 4.3. Graph Convolution Module ‣ 4. Framework of MTGNN ‣ Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks\"), the parameter matrix 𝐖(k)superscript 𝐖 𝑘\\mathbf{W}^{(k)} functions as a feature selector. When the given graph structure does not entail spatial dependencies, Equation [8](https://ar5iv.org/html/2005.11650#S4.E8 \"In Mix-hop Propagation Layer. ‣ 4.3. Graph Convolution Module ‣ 4. Framework of MTGNN ‣ Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks\") is still able to preserve the original node-self information by adjusting 𝐖(k)superscript 𝐖 𝑘\\mathbf{W}^{(k)} to 0 for all k>0 𝑘 0 k>0.\n",
      "\n",
      "Connection to existing works. The idea of mix-hop has been explored by (Kapoor et al., [2019](https://ar5iv.org/html/2005.11650#bib.bib10)) and (Chen et al., [2019b](https://ar5iv.org/html/2005.11650#bib.bib3)). Kapoor et al. (Kapoor et al., [2019](https://ar5iv.org/html/2005.11650#bib.bib10)) concatenate information from different hops. Chen et al. (Chen et al., [2019b](https://ar5iv.org/html/2005.11650#bib.bib3)) propose an attention mechanism to weight information among different hops. They both apply GCN for information propagation. However, as GCN faces the over-smoothing problem, information from higher hops may not or negatively contribute to the overall performance. To avoid this, our approach keeps a balance between local and neighborhood information. Furthermore, Kapoor et al. (Kapoor et al., [2019](https://ar5iv.org/html/2005.11650#bib.bib10)) show that their proposed model with two mix-hop layers has the capability to represent the delta difference between two consecutive hops. Our approach can achieve the same effect with only one mix-hop propagation layer. Suppose K=2 𝐾 2 K=2, 𝐖(0)=𝟎 superscript 𝐖 0 0\\mathbf{W}^{(0)}=\\mathbf{0}, 𝐖(1)=−𝟏 superscript 𝐖 1 1\\mathbf{W}^{(1)}=-\\mathbf{1}, and 𝐖(2)=𝟏 superscript 𝐖 2 1\\mathbf{W}^{(2)}=\\mathbf{1}, then\n",
      "\n",
      "(9)𝐇 o​u​t=Δ​(𝐇(2),𝐇(1))=𝐇 2−𝐇 1.subscript 𝐇 𝑜 𝑢 𝑡 Δ superscript 𝐇 2 superscript 𝐇 1 superscript 𝐇 2 superscript 𝐇 1\\mathbf{H}_{out}=\\Delta(\\mathbf{H}^{(2)},\\mathbf{H}^{(1)})=\\mathbf{H}^{2}-\\mathbf{H}^{1}.\n",
      "\n",
      "From this perspective, using summation is more efficient to represent all linear interactions of different hops compared with the concatenation method.\n",
      "\n",
      "![Image 6: Refer to caption](https://ar5iv.labs.arxiv.org/html/2005.11650/assets/x6.png)\n",
      "\n",
      "(a)TC module\n",
      "\n",
      "![Image 7: Refer to caption](https://ar5iv.labs.arxiv.org/html/2005.11650/assets/x7.png)\n",
      "\n",
      "(b)Dilated inception layer\n",
      "\n",
      "Figure 5. The temporal convolution and dilated inception layer.\n",
      "\n",
      "### 4.4. Temporal Convolution Module\n",
      "\n",
      "The temporal convolution module applies a set of standard dilated 1D convolution filters to extract high-level temporal features. This module consists of two dilated inception layers. One dilated inception layer is followed by a tangent hyperbolic activation function and works as a filter. The other layer is followed by a sigmoid activation function and functions as a gate to control the amount of information that the filter can pass to the next module. Figure [5](https://ar5iv.org/html/2005.11650#S4.F5 \"Figure 5 ‣ Mix-hop Propagation Layer. ‣ 4.3. Graph Convolution Module ‣ 4. Framework of MTGNN ‣ Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks\") shows the architecture of the temporal convolution module and the dilated inception layer.\n",
      "\n",
      "##### Dilated Inception Layer\n",
      "\n",
      "The temporal convolution module captures sequential patterns of time series data through 1D convolutional filters. To come up with a temporal convolution module that is able to both discover temporal patterns with various ranges and handle very long sequences, we propose the dilated inception layer which combines two widely applied strategies from convolutional neural networks, i.e., using filters with multiple sizes (Szegedy et al., [2015](https://ar5iv.org/html/2005.11650#bib.bib21)) and applying dilated convolution (Yu and Koltun, [2016](https://ar5iv.org/html/2005.11650#bib.bib25)).\n",
      "\n",
      "First, choosing the right kernel size is a challenging problem for convolutional networks. The filter size can be too large to represent short-term signal patterns subtly, or too small to discover long-term signal patterns sufficiently. In image processing, a widely employed strategy is called inception, which concatenates the outputs of 2D convolution filters with three different kernel sizes, 1×1 1 1 1\\times 1, 3×3 3 3 3\\times 3, and 5×5 5 5 5\\times 5. Moving from 2D images to 1D time series, the set of 1×1 1 1 1\\times 1, 1×3 1 3 1\\times 3, and 1×5 1 5 1\\times 5 filter sizes do not suit the nature of temporal signals. As temporal signals tend to have several inherent periods such as 7 7 7, 12 12 12, 24 24 24, 28 28 28, and 60 60 60, a stack of inception layers with filter size 1×1 1 1 1\\times 1, 1×3 1 3 1\\times 3, and 1×5 1 5 1\\times 5 cannot well encompass those periods. Alternatively, we propose a temporal inception layer consisting of four filter sizes, viz. 1×2 1 2 1\\times 2, 1×3 1 3 1\\times 3, 1×6 1 6 1\\times 6, and 1×7 1 7 1\\times 7. The aforementioned periods can all be covered by the combination of these filter sizes. For example, to represent the period 12 12 12, a model can pass the inputs through a 1×7 1 7 1\\times 7 filter from the first temporal inception layer followed by a 1×6 1 6 1\\times 6 filter from the second temporal inception layer.\n",
      "\n",
      "Second, the receptive field size of a convolutional network grows in a linear progression with the depth of the network and the kernel size of the filter. Consider a convolutional network with m 𝑚 m 1​D 1 𝐷 1D convolution layers of kernel size c 𝑐 c, the receptive field size of the convolutional network is,\n",
      "\n",
      "(10)R=m​(c−1)+1.𝑅 𝑚 𝑐 1 1 R=m(c-1)+1.\n",
      "\n",
      "To process very long sequences, it requires either a very deep network or very large filters. We adopt dilated convolution to reduce model complexity. Dilated convolution operates a standard convolution filter on down-sampled inputs with a certain frequency. For example, where the dilation factor is 2, it applies standard convolution on inputs sampled every two steps. Following (Oord et al., [2016](https://ar5iv.org/html/2005.11650#bib.bib15)), we let the dilation factor for each layer increase exponentially at a rate of q​(q>1)𝑞 𝑞 1 q\\;\\;(q>1). Suppose the initial dilation factor is 1 1 1, the receptive field size of a m 𝑚 m layer dilated convolutional network with kernel size c 𝑐 c is\n",
      "\n",
      "(11)R=1+(c−1)​(q m−1)/(q−1).𝑅 1 𝑐 1 superscript 𝑞 𝑚 1 𝑞 1 R=1+(c-1)(q^{m}-1)/(q-1).\n",
      "\n",
      "This indicates that the receptive field size of the network also grows exponentially with an increase in the number of hidden layers at the rate of q 𝑞 q. Therefore, using this dilation strategy can capture much longer sequences than proceeding without it.\n",
      "\n",
      "Formally, combining inception and dilation, we propose the dilated inception layer, demonstrated by Figure [5(b)](https://ar5iv.org/html/2005.11650#S4.F5.sf2 \"In Figure 5 ‣ Mix-hop Propagation Layer. ‣ 4.3. Graph Convolution Module ‣ 4. Framework of MTGNN ‣ Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks\"). Given a 1D sequence input 𝐳∈𝐑 T 𝐳 superscript 𝐑 𝑇\\mathbf{z}\\in\\mathbf{R}^{T} and filters consisting of 𝐟 1×2∈𝐑 2 subscript 𝐟 1 2 superscript 𝐑 2\\mathbf{f}_{1\\times 2}\\in\\mathbf{R}^{2}, 𝐟 1×3∈𝐑 3 subscript 𝐟 1 3 superscript 𝐑 3\\mathbf{f}_{1\\times 3}\\in\\mathbf{R}^{3}, 𝐟 1×6∈𝐑 6 subscript 𝐟 1 6 superscript 𝐑 6\\mathbf{f}_{1\\times 6}\\in\\mathbf{R}^{6}, and 𝐟 1×7∈𝐑 7 subscript 𝐟 1 7 superscript 𝐑 7\\mathbf{f}_{1\\times 7}\\in\\mathbf{R}^{7}, our dilated inception layer takes the form,\n",
      "\n",
      "(12)𝐳=c​o​n​c​a​t​(𝐳⋆𝐟 1×2,𝐳⋆𝐟 1×3,𝐳⋆𝐟 1×6,𝐳⋆𝐟 1×7),𝐳 𝑐 𝑜 𝑛 𝑐 𝑎 𝑡⋆𝐳 subscript 𝐟 1 2⋆𝐳 subscript 𝐟 1 3⋆𝐳 subscript 𝐟 1 6⋆𝐳 subscript 𝐟 1 7\\mathbf{z}=concat(\\mathbf{z}\\star\\mathbf{f}_{1\\times 2},\\mathbf{z}\\star\\mathbf{f}_{1\\times 3},\\mathbf{z}\\star\\mathbf{f}_{1\\times 6},\\mathbf{z}\\star\\mathbf{f}_{1\\times 7}),\n",
      "\n",
      "where the outputs of the four filters are truncated to the same length according to the largest filter and concatenated across the channel dimension, and the dilated convolution denoted by 𝐳⋆𝐟 1×k⋆𝐳 subscript 𝐟 1 𝑘\\mathbf{z}\\star\\mathbf{f}_{1\\times k} is defined as\n",
      "\n",
      "(13)𝐳⋆𝐟 1×k​(t)=∑s=0 k−1 𝐟 1×k​(s)​𝐳​(t−d×s),⋆𝐳 subscript 𝐟 1 𝑘 𝑡 superscript subscript 𝑠 0 𝑘 1 subscript 𝐟 1 𝑘 𝑠 𝐳 𝑡 𝑑 𝑠\\mathbf{z}\\star\\mathbf{f}_{1\\times k}(t)=\\sum_{s=0}^{k-1}\\mathbf{f}_{1\\times k}(s)\\mathbf{z}(t-d\\times s),\n",
      "\n",
      "where d 𝑑 d is the dilation factor.\n",
      "\n",
      "### 4.5. Skip Connection Layer & Output Module\n",
      "\n",
      "Skip connection layers are essentially 1×L i 1 subscript 𝐿 𝑖 1\\times L_{i} standard convolutions where L i subscript 𝐿 𝑖 L_{i} is the sequence length of the inputs to the i t​h superscript 𝑖 𝑡 ℎ i^{th} skip connection layer. It standardizes information that jumps to the output module to have the same sequence length 1 1 1. The output module consists of two 1×1 1 1 1\\times 1 standard convolution layers, transforming the channel dimension of the inputs to the desired output dimension. In case we want to predict a certain future step only, the desired output dimension is 1 1 1. When we want to predict Q 𝑄 Q consecutive steps, the desired output dimension is Q 𝑄 Q.\n",
      "\n",
      "### 4.6. Proposed Learning Algorithm\n",
      "\n",
      "We propose a learning algorithm to enhance our model’s capability of handling large graphs and stabilizing in a better local optimum. Training on a graph often requires storing all node intermediate states into memory. If a graph is large, it will face the problem of memory overflow. Most relevant to us, Chiang et al. (Chiang et al., [2019](https://ar5iv.org/html/2005.11650#bib.bib5)) propose a sub-graph training algorithm to tackle the memory bottleneck. They apply a graph clustering algorithm to partition a graph into sub-graphs and train a graph convolutional network on the partitioned sub-graphs. In our problem, it is not practical to cluster nodes based on their topological information because our model learns the latent graph structure at the same time. Alternatively, in each iteration, we randomly split the nodes into several groups and let the algorithm learn a sub-graph structure based on the sampled nodes. This gives each node the full possibilities of being assigned with another node in one group so that the similarity score between these two nodes can be computed and updated. As a side benefit, if we split the nodes into s 𝑠 s groups, we can reduce the time and space complexity of our graph learning layer from O​(N 2)𝑂 superscript 𝑁 2 O(N^{2}) to (N/s)2 superscript 𝑁 𝑠 2(N/s)^{2} in each iteration. After training, as all node embeddings are well-trained, a global graph can be constructed to fully utilize spatial relationships. Although it is computationally expensive, the adjacency matrix can be pre-computed in parallel before making predictions.\n",
      "\n",
      "The second consideration of our proposed algorithm is to facilitate our model stabilize in a better local optimum. In the task of multi-step forecasting, we observe that long-term predictions often achieve greater improvements than those in the short-term in terms of model performance. We believe the reason is that our model predicts multi-steps altogether, and long-term predictions produce a much higher loss than short-term predictions. As a result, to minimize the overall loss, the model focuses more on improving the accuracy of long-term predictions. To address this issue we propose a curriculum learning strategy for the multi-step forecasting task. The algorithm starts with solving the easiest problem, predicting the next one-step only. It is very advantageous for the model to find a good starting point. With the increase in iteration numbers, we increase the prediction length of the model gradually so that the model can learn the hard task step by step. Covering all this, our algorithm is given in Algorithm 1. Further complexity analysis of our model can be found in Appendix A.1.\n",
      "\n",
      "Algorithm 1 The learning algorithm of MTGNN.\n",
      "\n",
      "1:Input: The dataset\n",
      "\n",
      "O 𝑂 O\n",
      ", node set\n",
      "\n",
      "V 𝑉 V\n",
      ", the initialized MTGNN model\n",
      "\n",
      "f​(⋅)𝑓⋅f(\\cdot)\n",
      "with\n",
      "\n",
      "Θ Θ\\Theta\n",
      ", learning rate\n",
      "\n",
      "γ 𝛾\\gamma\n",
      ", batch size\n",
      "\n",
      "b 𝑏 b\n",
      ", step size\n",
      "\n",
      "s 𝑠 s\n",
      ", split size\n",
      "\n",
      "m 𝑚 m\n",
      "(default=1).\n",
      "\n",
      "2:set\n",
      "\n",
      "i​t​e​r=1,r=1 formulae-sequence 𝑖 𝑡 𝑒 𝑟 1 𝑟 1 iter=1,r=1\n",
      "\n",
      "3:repeat\n",
      "\n",
      "4:sample a batch\n",
      "\n",
      "(𝒳∈R b×T×N×D,𝒴∈R b×T′×N)formulae-sequence 𝒳 superscript 𝑅 𝑏 𝑇 𝑁 𝐷 𝒴 superscript 𝑅 𝑏 superscript 𝑇′𝑁(\\mathcal{X}\\in R^{b\\times T\\times N\\times D},\\mathcal{Y}\\in R^{b\\times T^{\\prime}\\times N})\n",
      "from\n",
      "\n",
      "O 𝑂 O\n",
      ".\n",
      "\n",
      "5:random split the node set\n",
      "\n",
      "V 𝑉 V\n",
      "into\n",
      "\n",
      "m 𝑚 m\n",
      "groups,\n",
      "\n",
      "∪i=1 m V i=V superscript subscript 𝑖 1 𝑚 subscript 𝑉 𝑖 𝑉\\cup_{i=1}^{m}V_{i}=V\n",
      ".\n",
      "\n",
      "6:if\n",
      "\n",
      "i t e r%s==0 iter\\%s==0\n",
      "and\n",
      "\n",
      "r<=T′𝑟 superscript 𝑇′r<=T^{\\prime}\n",
      "then\n",
      "\n",
      "7:\n",
      "\n",
      "r=r+1 𝑟 𝑟 1 r=r+1\n",
      "\n",
      "8:end if\n",
      "\n",
      "9:for i in 1:m do\n",
      "\n",
      "10:compute\n",
      "\n",
      "𝒴^=f​(𝒳​[:,:,i​d​(V i),:];𝚯)^𝒴 𝑓 𝒳::𝑖 𝑑 subscript 𝑉 𝑖:𝚯\\hat{\\mathcal{Y}}=f(\\mathcal{X}[:,:,id(V_{i}),:];\\mathbf{\\Theta})\n",
      "\n",
      "11:compute\n",
      "\n",
      "L=l o s s(𝒴^[:,:r,:],𝒴[:,:r,i d(V i)])L=loss(\\hat{\\mathcal{Y}}[:,:r,:],\\mathcal{Y}[:,:r,id(V_{i})])\n",
      "\n",
      "12:compute the stochastic gradient of\n",
      "\n",
      "Θ Θ\\Theta\n",
      "according to\n",
      "\n",
      "L 𝐿 L\n",
      ".\n",
      "\n",
      "13:update model parameters\n",
      "\n",
      "Θ Θ\\Theta\n",
      "according to their gradients and the learning rate\n",
      "\n",
      "γ 𝛾\\gamma\n",
      ".\n",
      "\n",
      "14:end for\n",
      "\n",
      "15:\n",
      "\n",
      "i​t​e​r=i​t​e​r+1 𝑖 𝑡 𝑒 𝑟 𝑖 𝑡 𝑒 𝑟 1 iter=iter+1\n",
      ".\n",
      "\n",
      "16:until convergence\n",
      "\n",
      "5. Experimental Studies\n",
      "-----------------------\n",
      "\n",
      "We validate MTGNN on two tasks - both single-step and multi-step forecasting. First, we compare the performance of MTGNN with other multivariate time series models on four benchmark datasets for multivariate time series forecasting, where the aim is to predict a single future step. Furthermore, to show how well MTGNN performs, compared with other spatial-temporal graph neural networks which, in contrast, use pre-defined graph structural information, we evaluate MTGNN on two benchmark datasets for spatial-temporal graph neural networks, where the aim is to predict multiple future steps. Further results on parameter study can be found in Appendix A.4.\n",
      "\n",
      "### 5.1. Experimental Setting\n",
      "\n",
      "In Table [1](https://ar5iv.org/html/2005.11650#S5.T1 \"Table 1 ‣ 5.1. Experimental Setting ‣ 5. Experimental Studies ‣ Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks\"), we summarize statistics of benchmark datasets. More details about the datasets is given in Appendix A.2. We use five evaluation metrics, including Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), Mean Absolute Percentage Error (MAPE), Root Relative Squared Error (RRSE), and Empirical Correlation Coefficient (CORR). For RMSE, MAE, MAPE, and RRSE, lower values are better. For CORR, higher values are better. Other experimental setups are given in Appendix A.3.\n",
      "\n",
      "Table 1. Dataset statistics.\n",
      "\n",
      "Datasets# Samples# Nodes Sample Rate Input Length Output Length\n",
      "traffic 17,544 862 1 hour 168 1\n",
      "solar-energy 52,560 137 10 minutes 168 1\n",
      "electricity 26,304 321 1 hour 168 1\n",
      "exchange-rate 7,588 8 1 day 168 1\n",
      "metr-la 34272 207 5 minutes 12 12\n",
      "pems-bay 52116 325 5 minutes 12 12\n",
      "\n",
      "### 5.2. Baseline Methods for Comparision\n",
      "\n",
      "MTGNN and MTGNN+sampling are our models to be evaluated. MTGNN is our proposed model. MTGNN+sampling is our proposed model trained on a sampled subset of a graph in each iteration. Baseline methods are summarized in the following:\n",
      "\n",
      "#### 5.2.1. Single-step forecasting\n",
      "\n",
      "*   •\n",
      "AR: An auto-regressive model.\n",
      "\n",
      "*   •\n",
      "VAR-MLP: A hybrid model of the multilayer perception (MLP) and auto-regressive model (VAR) (Zhang, [2003](https://ar5iv.org/html/2005.11650#bib.bib26)).\n",
      "\n",
      "*   •\n",
      "GP: A Gaussian Process time series model (Roberts et al., [2013](https://ar5iv.org/html/2005.11650#bib.bib17); Frigola-Alcalde, [2016](https://ar5iv.org/html/2005.11650#bib.bib7)).\n",
      "\n",
      "*   •\n",
      "RNN-GRU: A recurrent neural network with fully connected GRU hidden units.\n",
      "\n",
      "*   •\n",
      "LSTNet: A deep neural network, which combines convolutional neural networks and recurrent neural networks (Lai et al., [2018](https://ar5iv.org/html/2005.11650#bib.bib13)).\n",
      "\n",
      "*   •\n",
      "TPA-LSTM: An attention-recurrent neural network (Shih et al., [2019](https://ar5iv.org/html/2005.11650#bib.bib20)).\n",
      "\n",
      "#### 5.2.2. Multi-step forecasting\n",
      "\n",
      "*   •\n",
      "DCRNN: A diffusion convolutional recurrent neural network, which combines diffusion graph convolutions with recurrent neural networks (Li et al., [2018](https://ar5iv.org/html/2005.11650#bib.bib14)).\n",
      "\n",
      "*   •\n",
      "STGCN: A spatial-temporal graph convolutional network, which incorporates graph convolutions with 1D convolutions (Yu et al., [2018](https://ar5iv.org/html/2005.11650#bib.bib24)).\n",
      "\n",
      "*   •\n",
      "Graph WaveNet: A spatial-temporal graph convolutional network, which integrates diffusion graph convolutions with 1D dilated convolutions (Wu et al., [2019](https://ar5iv.org/html/2005.11650#bib.bib22)).\n",
      "\n",
      "*   •\n",
      "ST-MetaNet: A sequence-to-sequence architecture, which employs meta networks to generate parameters (Pan et al., [2019](https://ar5iv.org/html/2005.11650#bib.bib16)).\n",
      "\n",
      "*   •\n",
      "GMAN: A graph multi-attention network with spatial and temporal attentions (Zheng et al., [2020](https://ar5iv.org/html/2005.11650#bib.bib27)).\n",
      "\n",
      "*   •\n",
      "MRA-BGCN: A multi-range attentive bicomponent GCN (Chen et al., [2019a](https://ar5iv.org/html/2005.11650#bib.bib4)).\n",
      "\n",
      "### 5.3. Main Results\n",
      "\n",
      "Table 2. Baseline comparison under single-step forecasting for multivariate time series methods.\n",
      "\n",
      "Table [2](https://ar5iv.org/html/2005.11650#S5.T2 \"Table 2 ‣ 5.3. Main Results ‣ 5. Experimental Studies ‣ Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks\") and Table [3](https://ar5iv.org/html/2005.11650#S5.T3 \"Table 3 ‣ 5.3.2. Multi-step forecasting ‣ 5.3. Main Results ‣ 5. Experimental Studies ‣ Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks\") provide the main experimental results of MTGNN and MTGNN+sampling. We observe that MTGNN achieves state-of-the-art results on most of the tasks, and the performance of MTGNN only degrades marginal when it samples sub-graphs for training. In the following, we discuss experimental results of single-step and multi-step forecasting respectively.\n",
      "\n",
      "#### 5.3.1. Single-step forecasting\n",
      "\n",
      "In this experiment, we compare MTGNN with other multivariate time series models. Table [2](https://ar5iv.org/html/2005.11650#S5.T2 \"Table 2 ‣ 5.3. Main Results ‣ 5. Experimental Studies ‣ Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks\") shows the experimental results for the single-step forecasting task. In general, our MTGNN achieves state-of-the-art results over almost all horizons on Solar-Energy, Traffic, and Electricity data. In particular, on Traffic data, the improvement of MTGNN in terms of RSE is significant. MTGNN lowers down RSE by 7.24%, 3.88%, 4.83% over the horizons of 3, 12, 24 on the traffic data. The main reason why MTGNN improves the results of traffic data evidently is that the nature of traffic data is better suited for our model assumption about the spatial-temporal dependencies. Obviously, the future traffic occupancy rate of a road not only depends on its past but also on its connected roads’ occupancy rates. MTGNN fails to make improvements on the exchange-rate data, possibly due to the smaller graph size and fewer training examples of exchange-rate data.\n",
      "\n",
      "#### 5.3.2. Multi-step forecasting\n",
      "\n",
      "In this experiment, we compare MTGNN with other spatial-temporal graph neural network models. Table [3](https://ar5iv.org/html/2005.11650#S5.T3 \"Table 3 ‣ 5.3.2. Multi-step forecasting ‣ 5.3. Main Results ‣ 5. Experimental Studies ‣ Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks\") shows the experimental results for the task of multi-step forecasting. The significance of MTGNN lies in that it achieves on-par performance with state-of-the-art spatial-temporal graph neural networks without using a pre-defined graph, while DCRNN, STGCN, and MRA-BGCN fully rely on pre-defined graphs. Graph Wavenet proposes a self-adaptive adjacency matrix, but it needs to combine with a pre-defined graph in order to achieve optimal performance. ST-MetaNet employs attention mechanisms to adjust the edge weights of a pre-defined graph. GMAN leverages node2vec algorithm to preserve node structural information while performing attention mechanisms. When a graph is not defined, these methods cannot model multivariate times series data efficiently.\n",
      "\n",
      "Table 3. Baseline comparison under multi-step forecasting for spatial-temporal graph neural networks.\n",
      "\n",
      "### 5.4. Ablation Study\n",
      "\n",
      "We conduct an ablation study on the METR-LA data to validate the effectiveness of key components that contribute to the improved outcomes of our proposed model. We name MTGNN without different components as follows:\n",
      "\n",
      "*   •\n",
      "w/o GC: MTGNN without the graph convolution module. We replace the graph convolution module with a linear layer.\n",
      "\n",
      "*   •\n",
      "w/o Mix-hop: MTGNN without the information selection step in the mix-hop propagation layer. We pass the outputs of the information propagation step to the next module directly.\n",
      "\n",
      "*   •\n",
      "w/o Inception: MTGNN without inception in the dilated inception layer. While keeping the same number of output channels, we use a single 1×7 1 7 1\\times 7 filter only.\n",
      "\n",
      "*   •\n",
      "w/o CL: MTGNN without curriculum learning. We train MTGNN without gradually increasing the prediction length.\n",
      "\n",
      "We repeat each experiment 10 times with 50 epochs per repetition and report the average of MAE, RMSE, MAPE with a standard deviation over 10 runs on the validation set in Table [4](https://ar5iv.org/html/2005.11650#S5.T4 \"Table 4 ‣ 5.4. Ablation Study ‣ 5. Experimental Studies ‣ Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks\"). The introduction of graph convolution modules significantly improves the results as it enables information flow among isolated but interdependent nodes. The effect of mix-hop is evident as well: it validates that the use of mix-hop is helpful for selecting useful information at each information propagation step in the mix-hop propagation layer. The effect of inception is significant in terms of RMSE, but marginal in terms of MAE. This is because using a single 1×7 1 7 1\\times 7 filter has half more parameters than using a combination of 1×2,1×3,1×5,1×7 1 2 1 3 1 5 1 7 1\\times 2,1\\times 3,1\\times 5,1\\times 7 filters under the condition that the number of output channels for the dilated inception layer remains the same. Lastly, our curriculum learning strategy proves to be effective. It enables our model to converge quickly to an optimum that fits for the easiest task, and fine-tune parameters step by step as the level of learning difficulty increases.\n",
      "\n",
      "Table 4. Ablation study.\n",
      "\n",
      "Methods MTGNN w/o GC w/o Mix-hop w/o Inception w/o CL\n",
      "MAE 2.7715±plus-or-minus\\pm 0.0119 2.8953±plus-or-minus\\pm 0.0054 2.7975±plus-or-minus\\pm 0.0089 2.7772±plus-or-minus\\pm 0.0100 2.7828±plus-or-minus\\pm 0.0105\n",
      "RMSE 5.8070±plus-or-minus\\pm 0.0512 6.1276±plus-or-minus\\pm 0.0339 5.8549±plus-or-minus\\pm 0.0474 5.8251±plus-or-minus\\pm 0.0429 5.8248±plus-or-minus\\pm 0.0366\n",
      "MAPE 0.0778±plus-or-minus\\pm 0.0009 0.0831±plus-or-minus\\pm 0.0009 0.0779±plus-or-minus\\pm 0.0009 0.0778±plus-or-minus\\pm 0.0010 0.0784±plus-or-minus\\pm 0.0009\n",
      "\n",
      "### 5.5. Study of the Graph Learning Layer\n",
      "\n",
      "To validate the effectiveness of our proposed graph learning layer, we conduct a study which experiments with different ways of constructing a graph adjacency matrix. Table [5](https://ar5iv.org/html/2005.11650#S5.T5 \"Table 5 ‣ 5.5. Study of the Graph Learning Layer ‣ 5. Experimental Studies ‣ Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks\") shows different forms of 𝐀 𝐀\\mathbf{A} with experimental results tested on the validation set of the METR-LA data averaged on 10 runs. Predefined-A is constructed by road network distance (Li et al., [2018](https://ar5iv.org/html/2005.11650#bib.bib14)). Global-A assumes the adjacency matrix is a parameter matrix, which contains N 2 superscript 𝑁 2 N^{2} parameters. Motivated by (Wu et al., [2019](https://ar5iv.org/html/2005.11650#bib.bib22)), Undirected-A and Directed-A are computed by the similarity scores of node embeddings. Motivated by (Guo et al., [2019](https://ar5iv.org/html/2005.11650#bib.bib9); Shi et al., [2019](https://ar5iv.org/html/2005.11650#bib.bib19)), Dynamic-A assumes the spatial dependency at each time step is dependent on its node inputs. Uni-directed-A is our proposed method. According to Table [5](https://ar5iv.org/html/2005.11650#S5.T5 \"Table 5 ‣ 5.5. Study of the Graph Learning Layer ‣ 5. Experimental Studies ‣ Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks\"), our proposed uni-directed-A achieves the lowest mean MAE, RMSE, and MAPE. It improves over predefined-A, undirected-A, and dynamic-A significantly. Our uni-directed-A improves over undirected-A and directed-A marginally in terms of MAE and MAPE but proves to be more robust due to a lower RMSE.\n",
      "\n",
      "We further investigate the learned graph adjacency matrix via a case study. In Figure [6(a)](https://ar5iv.org/html/2005.11650#S5.F6.sf1 \"In Figure 6 ‣ 5.5. Study of the Graph Learning Layer ‣ 5. Experimental Studies ‣ Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks\"), we plot the raw time series of node 55 and its pre-defined top-3 neighbors. In Figure [6(b)](https://ar5iv.org/html/2005.11650#S5.F6.sf2 \"In Figure 6 ‣ 5.5. Study of the Graph Learning Layer ‣ 5. Experimental Studies ‣ Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks\"), we chart the raw time series of node 55 and its learned top-3 neighbors. Figure [6(c)](https://ar5iv.org/html/2005.11650#S5.F6.sf3 \"In Figure 6 ‣ 5.5. Study of the Graph Learning Layer ‣ 5. Experimental Studies ‣ Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks\") shows the geo-location of these nodes, with green nodes representing the central node’s learned top-3 neighbors and yellow nodes representing the central node’s pre-defined top-3 neighbors. We observe that the central node’s pre-defined top-3 neighbors are much closer to the node itself on the map. As a result, their time series are more correlated simultaneously, as shown by the red circles in Figure [6(a)](https://ar5iv.org/html/2005.11650#S5.F6.sf1 \"In Figure 6 ‣ 5.5. Study of the Graph Learning Layer ‣ 5. Experimental Studies ‣ Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks\"). On the contrary, the central node’s learned top-3 neighbors distribute further away from it but still lie on the same road it follows. According to Figure [6(b)](https://ar5iv.org/html/2005.11650#S5.F6.sf2 \"In Figure 6 ‣ 5.5. Study of the Graph Learning Layer ‣ 5. Experimental Studies ‣ Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks\"), time series of the learned top-3 neighbors are more capable of indicating extreme traffic conditions of the central node in advance.\n",
      "\n",
      "![Image 8: Refer to caption](https://ar5iv.labs.arxiv.org/html/2005.11650/assets/x8.png)\n",
      "\n",
      "(a)Time series of node 55 and its top-3 neighbors given by the pre-defined 𝐀 𝐀\\mathbf{A}.\n",
      "\n",
      "![Image 9: Refer to caption](https://ar5iv.labs.arxiv.org/html/2005.11650/assets/x9.png)\n",
      "\n",
      "(b)Time series of node 55 and its top-3 neighbors given by the learned 𝐀 𝐀\\mathbf{A}.\n",
      "\n",
      "![Image 10: Refer to caption](https://ar5iv.labs.arxiv.org/html/2005.11650/assets/x10.png)\n",
      "\n",
      "(c)Node locations of node 55 and its neighbors marked on Google Maps. Yellow nodes represent node 55’s top-3 neighbors given by the pre-defined 𝐀 𝐀\\mathbf{A}. Green nodes represent node 55’s top-3 neighbors given by the learned 𝐀 𝐀\\mathbf{A}.\n",
      "\n",
      "Figure 6. Case study\n",
      "\n",
      "Table 5. Comparison of different graph learning methods.\n",
      "\n",
      "6. Conclusions\n",
      "--------------\n",
      "\n",
      "In this paper, we introduce a novel framework for multivariate time series forecasting. To the best of our knowledge, we are the first to address the multivariate time series forecasting problem via a graph-based deep learning approach. We propose an effective method to exploit the inherent dependency relationships among multiple time series. Our method demonstrates superb performance in a variety of multivariate time series forecasting tasks and opens a new door to use GNNs to handle diverse non-structural data.\n",
      "\n",
      "###### Acknowledgements.\n",
      "\n",
      "This work was supported in part by the Australian Research Council (ARC) under Grant LP160100630, LP180100654 and DE190100626.\n",
      "\n",
      "References\n",
      "----------\n",
      "\n",
      "*   (1)\n",
      "*   Box et al. (2015) George EP Box, Gwilym M Jenkins, Gregory C Reinsel, and Greta M Ljung. 2015. _Time series analysis: forecasting and control_. John Wiley & Sons. \n",
      "*   Chen et al. (2019b) Fengwen Chen, Shirui Pan, Jing Jiang, Huan Huo, and Guodong Long. 2019b. DAGCN: Dual Attention Graph Convolutional Networks. In _Proc. of IJCNN_. \n",
      "*   Chen et al. (2019a) Weiqi Chen, Ling Chen, Yu Xie, Wei Cao, Yusong Gao, and Xiaojie Feng. 2019a. Multi-Range Attentive Bicomponent Graph Convolutional Network for Traffic Forecasting. In _Proc. of AAAI_. \n",
      "*   Chiang et al. (2019) Wei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, and Cho-Jui Hsieh. 2019. Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks. In _Proc. of KDD_. \n",
      "*   Frigola (2015) Roger Frigola. 2015. _Bayesian time series learning with Gaussian processes_. Ph.D. Dissertation. University of Cambridge. \n",
      "*   Frigola-Alcalde (2016) Roger Frigola-Alcalde. 2016. _Bayesian time series learning with Gaussian processes_. Ph.D. Dissertation. University of Cambridge. \n",
      "*   Gilmer et al. (2017) Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. 2017. Neural message passing for quantum chemistry. In _Proc. of ICML_. 1263–1272. \n",
      "*   Guo et al. (2019) Shengnan Guo, Youfang Lin, Ning Feng, Chao Song, and Huaiyu Wan. 2019. Attention based spatial-temporal graph convolutional networks for traffic flow forecasting. In _Proc. of AAAI_, Vol.33. 922–929. \n",
      "*   Kapoor et al. (2019) Amol Kapoor, Aram Galstyan, Bryan Perozzi, Greg Ver Steeg, Hrayr Harutyunyan, Kristina Lerman, Nazanin Alipourfard, and Sami Abu-El-Haija. 2019. MixHop: Higher-Order Graph Convolutional Architectures via Sparsified Neighborhood Mixing. In _Proc. of ICML_. \n",
      "*   Kipf and Welling (2017) Thomas N Kipf and Max Welling. 2017. Semi-supervised classification with graph convolutional networks. In _Proc. of ICLR_. \n",
      "*   Klicpera et al. (2019) Johannes Klicpera, Aleksandar Bojchevski, and Stephan Günnemann. 2019. Predict then propagate: Graph neural networks meet personalized pagerank. In _Proc. of ICLR_. \n",
      "*   Lai et al. (2018) Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. 2018. Modeling long-and short-term temporal patterns with deep neural networks. In _Proc. of SIGIR_. ACM, 95–104. \n",
      "*   Li et al. (2018) Yaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu. 2018. Diffusion convolutional recurrent neural network: Data-driven traffic forecasting. In _Proc. of ICLR_. \n",
      "*   Oord et al. (2016) Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. 2016. Wavenet: A generative model for raw audio. _arXiv:1609.03499_ (2016). \n",
      "*   Pan et al. (2019) Zheyi Pan, Yuxuan Liang, Weifeng Wang, Yong Yu, Yu Zheng, and Junbo Zhang. 2019. Urban Traffic Prediction from Spatio-Temporal Data Using Deep Meta Learning. In _KDD_. ACM, 1720–1730. \n",
      "*   Roberts et al. (2013) Stephen Roberts, Michael Osborne, Mark Ebden, Steven Reece, Neale Gibson, and Suzanne Aigrain. 2013. Gaussian processes for time-series modelling. _Philos. Trans. R. Soc. A_ 371, 1984 (2013), 20110550. \n",
      "*   Seo et al. (2018) Youngjoo Seo, Michaël Defferrard, Pierre Vandergheynst, and Xavier Bresson. 2018. Structured sequence modeling with graph convolutional recurrent networks. In _Proc. of NIPS_. Springer, 362–373. \n",
      "*   Shi et al. (2019) Lei Shi, Yifan Zhang, Jian Cheng, and Hanqing Lu. 2019. Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition. In _Proc. of CVPR_. 12026–12035. \n",
      "*   Shih et al. (2019) Shun-Yao Shih, Fan-Keng Sun, and Hung-yi Lee. 2019. Temporal pattern attention for multivariate time series forecasting. _Machine Learning_ 108, 8-9 (2019), 1421–1441. \n",
      "*   Szegedy et al. (2015) Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. 2015. Going deeper with convolutions. In _Proc. of CVPR_. 1–9. \n",
      "*   Wu et al. (2019) Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, and Chengqi Zhang. 2019. Graph WaveNet for Deep Spatial-Temporal Graph Modeling. In _Proc. of IJCAI_. \n",
      "*   Yan et al. (2018) Sijie Yan, Yuanjun Xiong, and Dahua Lin. 2018. Spatial temporal graph convolutional networks for skeleton-based action recognition. In _Proc. of AAAI_. 3482–3489. \n",
      "*   Yu et al. (2018) Bing Yu, Haoteng Yin, and Zhanxing Zhu. 2018. Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting. In _Proc. of IJCAI_. 3634–3640. \n",
      "*   Yu and Koltun (2016) Fisher Yu and Vladlen Koltun. 2016. Multi-scale context aggregation by dilated convolutions. In _ICLR_. \n",
      "*   Zhang (2003) G Peter Zhang. 2003. Time series forecasting using a hybrid ARIMA and neural network model. _Neurocomputing_ 50 (2003), 159–175. \n",
      "*   Zheng et al. (2020) Chuanpan Zheng, Xiaoliang Fan, Cheng Wang, and Jianzhong Qi. 2020. GMAN: A Graph Multi-Attention Network for Traffic Prediction. In _Proc. of AAAI_. \n",
      "\n",
      "Appendix A APPENDIX: REPRODUCIBILITY\n",
      "------------------------------------\n",
      "\n",
      "In this section, we provide the details of our implementation for reproducibility. Our source codes 1 1 1 https://github.com/nnzhan/MTGNN are publicly available.\n",
      "\n",
      "### A.1. Complexity Analysis\n",
      "\n",
      "We analyze the time complexity of the main components of the proposed model MTGNN, which is summarized in Table [6](https://ar5iv.org/html/2005.11650#A1.T6 \"Table 6 ‣ A.1. Complexity Analysis ‣ Appendix A APPENDIX: REPRODUCIBILITY ‣ Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks\"). The time complexity of the graph learning layer is (O​(N​s 1​s 2+N 2​s 2))𝑂 𝑁 subscript 𝑠 1 subscript 𝑠 2 superscript 𝑁 2 subscript 𝑠 2(O(Ns_{1}s_{2}+N^{2}s_{2})) where N 𝑁 N denotes the number of nodes, s 1 subscript 𝑠 1 s_{1} represents the dimension of node input feature vectors, and s 2 subscript 𝑠 2 s_{2} represents the dimension of node hidden feature vectors. Treating s 1 subscript 𝑠 1 s_{1} and s 2 subscript 𝑠 2 s_{2} as constants, the time complexity of the graph learning layer becomes O​(N 2)𝑂 superscript 𝑁 2 O(N^{2}). It is attributed to the pairwise computation of node hidden feature vectors. The graph convolution module incurs O(K(M d 1+N d 1 d 2)O(K(Md_{1}+Nd_{1}d_{2}) time complexity, where K 𝐾 K is the propagation depth, N 𝑁 N is the number of nodes, d 1 subscript 𝑑 1 d_{1} denotes the input dimension of node states, d 2 subscript 𝑑 2 d_{2} denotes the output dimension of node states. Regarding K 𝐾 K, d 1 subscript 𝑑 1 d_{1} and d 2 subscript 𝑑 2 d_{2} as constants, the time complexity of the graph convolution module turns to O​(M)𝑂 𝑀 O(M). This result comes from the fact that in the information propagation step, each node receives information from its neighbors and the sum of the number of neighbors of each node exactly equals the number of edges. The time complexity of the temporal convolution module equals to O​(N​l​c i​c o/d)𝑂 𝑁 𝑙 subscript 𝑐 𝑖 subscript 𝑐 𝑜 𝑑 O(Nlc_{i}c_{o}/d), where l 𝑙 l is the input sequence length, c i subscript 𝑐 𝑖 c_{i} is the number of input channels, c o subscript 𝑐 𝑜 c_{o} is the number of output channels, and d 𝑑 d is the dilation factor. The time complexity of the temporal convolution module mainly depends on N×l 𝑁 𝑙 N\\times l, which is the size of the input feature map.\n",
      "\n",
      "Table 6. Time Complexity Analysis\n",
      "\n",
      "### A.2. Data\n",
      "\n",
      "In Table [1](https://ar5iv.org/html/2005.11650#S5.T1 \"Table 1 ‣ 5.1. Experimental Setting ‣ 5. Experimental Studies ‣ Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks\"), we summarize statistics of benchmark datasets. Details of these datasets are introduced below.\n",
      "\n",
      "#### A.2.1. Single-step forecasting\n",
      "\n",
      "*   •\n",
      "Traffic: the traffic dataset from the California Department of Transportation contains road occupancy rates measured by 862 sensors in San Francisco Bay area freeways during 2015 and 2016.\n",
      "\n",
      "*   •\n",
      "Solar-Energy: the solar-energy dataset from the National Renewable Energy Laboratory contains the solar power output collected from 137 PV plants in Alabama State in 2007.\n",
      "\n",
      "*   •\n",
      "Electricity: the electricity dataset from the UCI Machine Learning Repository contains electricity consumption for 321 clients from 2012 to 2014.\n",
      "\n",
      "*   •\n",
      "Exchange-Rate: the exchange-rate dataset contains the daily exchange rates of eight foreign countries including Australia, British, Canada, Switzerland, China, Japan, New Zealand, and Singapore ranging from 1990 to 2016.\n",
      "\n",
      "Following (Lai et al., [2018](https://ar5iv.org/html/2005.11650#bib.bib13)), we split these four datasets into a training set (60%), validation set (20%), and test set (20%) in chronological order. The input sequence length is 168 and the output sequence length is 1. Models are trained independently to predict the target future step (horizon) 3, 6, 12, and 24.\n",
      "\n",
      "#### A.2.2. Multi-step forecasting\n",
      "\n",
      "*   •\n",
      "METR-LA: the METR-LA dataset from the Los Angeles Metropolitan Transportation Authority contains average traffic speed measured by 207 loop detectors on the highways of Los Angeles County ranging from Mar 2012 to Jun 2012.\n",
      "\n",
      "*   •\n",
      "PEMS-BAY: the PEMS-BAY dataset from California Transportation Agencies (CalTrans) contains average traffic speed measured by 325 sensors in the Bay Area ranging from Jan 2017 to May 2017.\n",
      "\n",
      "Following (Li et al., [2018](https://ar5iv.org/html/2005.11650#bib.bib14)), we split these two datasets into a training set (70%), validation set (20%), and test set (10%) in chronological order. The input sequence length is 12, and the target sequence contains the next 12 future steps. The time of the day is used as an auxiliary feature for the inputs. For the selected baseline methods, the pairwise road network distances are used as the pre-defined graph structure.\n",
      "\n",
      "### A.3. Experimental Setup\n",
      "\n",
      "We repeat the experiment 10 times and report the average value of evaluation metrics. The model is trained by the Adam optimizer with gradient clip 5. The learning rate is 0.001. The l2 regularization penalty is 0.0001. Dropout with 0.3 is applied after each temporal convolution module. Layernorm is applied after each graph convolution module. The depth of the mix-hop propagation layer is set to 2. The retain ratio from the mix-hop propagation layer is set to 0.05 0.05 0.05. The saturation rate of the activation function from the graph learning layer is set to 3 3 3. The dimension of node embeddings is 40. Other hyper-parameters are reported according to different tasks.\n",
      "\n",
      "#### A.3.1. Single-step forecasting\n",
      "\n",
      "We use 5 graph convolution modules and 5 temporal convolution modules with the dilation exponential factor 2. The starting 1×1 1 1 1\\times 1 convolution has 1 input channel and 16 output channels. The graph convolution module and the temporal convolution modules both have 16 output channels. The skip connection layers all have 32 output channels. The first layer of the output module has 64 output channels and the second layer of the output module has 1 output channel. The number of training epochs is 30. For Traffic, Solar-Energy, and Electricity, the number of neighbors for each node is 20. For Exchange-Rate, the number of neighbors for each node is 8. The batch size is set to 4. For the MTGNN+sampling model, we split the nodes of a graph into three partitions randomly with a batch size of 16. Following (Lai et al., [2018](https://ar5iv.org/html/2005.11650#bib.bib13)), we use RSE and CORR as evaluation metrics.\n",
      "\n",
      "#### A.3.2. Multi-step forecasting\n",
      "\n",
      "We use 3 graph convolution modules and 3 temporal convolution modules with the dilation exponential factor 1. The starting 1×1 1 1 1\\times 1 convolution has 2 input channels and 32 output channels. The graph convolution module and the temporal convolution module both have 32 output channels. The skip connection layers all have 64 output channels. The first layer of the output module has 128 output channels and its second layer has 12 output channels. The number of neighbors for each node is 20. The number of training epochs is 100. The batch size is set to 64. Following (Li et al., [2018](https://ar5iv.org/html/2005.11650#bib.bib14)), we use MAE, RMSE , and MAPE as evaluation metrics.\n",
      "\n",
      "### A.4. Parameter Study\n",
      "\n",
      "We conduct a parameter study on eight core hyper-parameters which influence the model complexity of MTGNN. We list these hyper-parameters as follows: Number of layers, the number of temporal convolution modules, ranges from 1 to 6. Number of filters, the number of output channels for temporal convolution modules and graph convolution modules, ranges from 4 to 128. Number of neighbors, the parameter k 𝑘 k in Equation [5](https://ar5iv.org/html/2005.11650#S4.E5 \"In 4.2. Graph Learning Layer ‣ 4. Framework of MTGNN ‣ Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks\"), ranges from 10 to 60. Saturation rate, the parameter α 𝛼\\alpha in Equation [1](https://ar5iv.org/html/2005.11650#S4.E1 \"In 4.2. Graph Learning Layer ‣ 4. Framework of MTGNN ‣ Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks\"), [2](https://ar5iv.org/html/2005.11650#S4.E2 \"In 4.2. Graph Learning Layer ‣ 4. Framework of MTGNN ‣ Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks\"), and [3](https://ar5iv.org/html/2005.11650#S4.E3 \"In 4.2. Graph Learning Layer ‣ 4. Framework of MTGNN ‣ Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks\"), ranges from 0.5 to 5. Retain ratio of mix-hop propagation layer, the parameter β 𝛽\\beta in Equation [7](https://ar5iv.org/html/2005.11650#S4.E7 \"In Mix-hop Propagation Layer. ‣ 4.3. Graph Convolution Module ‣ 4. Framework of MTGNN ‣ Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks\"), ranges from 0 to 0.8. Depth of mix-hop propagation layer, the parameter K 𝐾 K in Equation [8](https://ar5iv.org/html/2005.11650#S4.E8 \"In Mix-hop Propagation Layer. ‣ 4.3. Graph Convolution Module ‣ 4. Framework of MTGNN ‣ Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks\"), ranges from 1 to 6.\n",
      "\n",
      "We repeat each experiment 10 times with 50 epochs each time and report the average of MAE with a standard deviation over 10 runs on the validation set. We change the parameter under investigation and fix other parameters in each experiment. Figure [7](https://ar5iv.org/html/2005.11650#A1.F7 \"Figure 7 ‣ A.4. Parameter Study ‣ Appendix A APPENDIX: REPRODUCIBILITY ‣ Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks\") shows the experimental results of our parameter study. As shown in Figure [7(a)](https://ar5iv.org/html/2005.11650#A1.F7.sf1 \"In Figure 7 ‣ A.4. Parameter Study ‣ Appendix A APPENDIX: REPRODUCIBILITY ‣ Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks\") and Figure [7(b)](https://ar5iv.org/html/2005.11650#A1.F7.sf2 \"In Figure 7 ‣ A.4. Parameter Study ‣ Appendix A APPENDIX: REPRODUCIBILITY ‣ Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks\"), increasing the number of layers and filters enhances our model’s expressive capacity, while reducing the MAE loss. Figure [7(c)](https://ar5iv.org/html/2005.11650#A1.F7.sf3 \"In Figure 7 ‣ A.4. Parameter Study ‣ Appendix A APPENDIX: REPRODUCIBILITY ‣ Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks\") shows that a small number of neighbors gives better results. It is possibly because a node may only depend on a limited number of other nodes, and increasing its neighborhood merely introduces noises to the model. The model performance is not sensitive to the saturation rate, as shown in Figure [7(d)](https://ar5iv.org/html/2005.11650#A1.F7.sf4 \"In Figure 7 ‣ A.4. Parameter Study ‣ Appendix A APPENDIX: REPRODUCIBILITY ‣ Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks\"). However, a large saturation rate can impose values of the adjacency matrix produced by the graph learning layer approach to 0 or 1. As shown in Figure [7(e)](https://ar5iv.org/html/2005.11650#A1.F7.sf5 \"In Figure 7 ‣ A.4. Parameter Study ‣ Appendix A APPENDIX: REPRODUCIBILITY ‣ Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks\"), a high retain ratio degrades the model performance significantly. We think it is because by default the propagation depth of the mix-hop propagation layer is set to 2 2 2, and as a result, keeping a high proportion of root information constrains a node from exploring its neighborhood. Figure [7(f)](https://ar5iv.org/html/2005.11650#A1.F7.sf6 \"In Figure 7 ‣ A.4. Parameter Study ‣ Appendix A APPENDIX: REPRODUCIBILITY ‣ Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks\") shows that it is enough to propagate node information with 2 or 3 steps. With the increase of the depth of propagation, the proposed mix-hop propagation layer does not suffer from the over-smoothing problem incurred by information aggregation. With the depth of propagation equal to 6, it has the lowest mean MAE with a larger variation.\n",
      "\n",
      "![Image 11: Refer to caption](https://ar5iv.labs.arxiv.org/html/2005.11650/assets/x11.png)\n",
      "\n",
      "(a)Number of layers\n",
      "\n",
      "![Image 12: Refer to caption](https://ar5iv.labs.arxiv.org/html/2005.11650/assets/x12.png)\n",
      "\n",
      "(b)Number of filters\n",
      "\n",
      "![Image 13: Refer to caption](https://ar5iv.labs.arxiv.org/html/2005.11650/assets/x13.png)\n",
      "\n",
      "(c)Number of neighbors\n",
      "\n",
      "![Image 14: Refer to caption](https://ar5iv.labs.arxiv.org/html/2005.11650/assets/x14.png)\n",
      "\n",
      "(d)Saturation rate\n",
      "\n",
      "![Image 15: Refer to caption](https://ar5iv.labs.arxiv.org/html/2005.11650/assets/x15.png)\n",
      "\n",
      "(e)Retain ratio\n",
      "\n",
      "![Image 16: Refer to caption](https://ar5iv.labs.arxiv.org/html/2005.11650/assets/x16.png)\n",
      "\n",
      "(f)Depth of propagation\n",
      "\n",
      "Figure 7. Parameter Study\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0425a0d9-650b-4d38-97b4-10419b5359de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2777c90a-deab-4ba7-a7d8-ed974512f9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove mathematical content\n",
    "_MATH_ENV_NAMES = [\n",
    "    \"equation\", \"equation*\", \"align\", \"align*\", \"aligned\",\n",
    "    \"gather\", \"gather*\", \"multline\", \"multline*\", \"eqnarray\", \"eqnarray*\",\n",
    "    \"split\", \"cases\", \"array\", \"pmatrix\", \"bmatrix\", \"vmatrix\", \"Vmatrix\"\n",
    "]\n",
    "\n",
    "def strip_equations(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove (La)TeX-style math from plain text:\n",
    "      - \\\\begin{equation}...\\\\end{equation}, align, gather, etc.\n",
    "      - $$...$$ display math\n",
    "      - \\\\[ ... \\\\] display math\n",
    "      - \\\\( ... \\\\) inline math\n",
    "      - $...$ inline math\n",
    "\n",
    "    Returns text with math removed and whitespace collapsed.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return text\n",
    "\n",
    "    # 1) Remove \\begin{...} ... \\end{...} blocks for common math envs\n",
    "    for env in _MATH_ENV_NAMES:\n",
    "        pattern = rf\"\\\\begin\\{{{env}\\}}.*?\\\\end\\{{{env}\\}}\"\n",
    "        text = re.sub(pattern, \" \", text, flags=re.DOTALL)\n",
    "\n",
    "    # 2) Remove display math: $$ ... $$\n",
    "    text = re.sub(r\"\\$\\$.*?\\$\\$\", \" \", text, flags=re.DOTALL)\n",
    "\n",
    "    # 3) Remove display math: \\[ ... \\]\n",
    "    text = re.sub(r\"\\\\\\[(.*?)\\\\\\]\", \" \", text, flags=re.DOTALL)\n",
    "\n",
    "    # 4) Remove inline math: \\( ... \\)\n",
    "    text = re.sub(r\"\\\\\\((.*?)\\\\\\)\", \" \", text, flags=re.DOTALL)\n",
    "\n",
    "    # 5) Remove inline math: $ ... $   (avoid greediness / nested $)\n",
    "    #    This matches a single pair of $...$ without spanning another $\n",
    "    text = re.sub(r\"\\$(?:\\\\.|[^$\\\\])+\\$\", \" \", text, flags=re.DOTALL)\n",
    "\n",
    "    # 6) Clean up stray math delimiters if any remain\n",
    "    text = re.sub(r\"[\\\\$]\", \" \", text)\n",
    "\n",
    "    # 7) Collapse whitespace/newlines\n",
    "    text = re.sub(r\"\\s+\\n\\s+\", \"\\n\\n\", text)         # keep paragraph breaks\n",
    "    text = re.sub(r\"[ \\t]{2,}\", \" \", text)           # collapse long spaces\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)           # max two newlines in a row\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def prepare_text_for_llm(raw_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Convenience preprocessor: strip equations and tidy whitespace.\n",
    "    \"\"\"\n",
    "    cleaned = strip_equations(raw_text)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d24d315-6208-4755-9a82-6edbd7ff26ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(\n",
    "        seq: Iterable[Any],\n",
    "        size: int,\n",
    "        step: int\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create overlapping chunks from a sequence using a sliding window approach.\n",
    "\n",
    "    Minor change: early return for empty input to avoid producing a single empty chunk.\n",
    "\n",
    "    Args:\n",
    "        seq: The input sequence (string or list) to be chunked.\n",
    "        size (int): The size of each chunk/window.\n",
    "        step (int): The step size between consecutive windows.\n",
    "\n",
    "    Returns:\n",
    "        list of dicts: [{'start': int, 'content': <slice>}, ...]\n",
    "\n",
    "    Example:\n",
    "        >>> sliding_window(\"hello world\", size=5, step=3)\n",
    "        [{'start': 0, 'content': 'hello'}, {'start': 3, 'content': 'lo wo'}]\n",
    "    \"\"\"\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "\n",
    "    # ---- Minor change: guard for empty seq ----\n",
    "    n = len(seq)  # works for str and list\n",
    "    if n == 0:\n",
    "        return []\n",
    "\n",
    "    result = []\n",
    "    for i in range(0, n, step):\n",
    "        batch = seq[i:i+size]\n",
    "        result.append({'start': i, 'content': batch})\n",
    "        if i + size > n:\n",
    "            break\n",
    "\n",
    "    return result\n",
    "\n",
    "# Perhaps I should do per-paragraph chunking???\n",
    "def chunk_documents(\n",
    "        documents: Iterable[Dict[str, str]],\n",
    "        size: int = 2000,\n",
    "        step: int = 1000,\n",
    "        content_field_name: str = 'content',\n",
    "        min_chars: int = 0,      # <-- Minor change: drop tiny tail chunks if desired\n",
    ") -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Split documents into overlapping chunks while preserving metadata.\n",
    "\n",
    "    Minor change: added `min_chars` to skip very small chunks that are often\n",
    "    just headers or stubs after math removal. Set to 0 to keep all.\n",
    "\n",
    "    Returns:\n",
    "        list of chunk dicts with original metadata + 'start' + 'content'\n",
    "    \"\"\"\n",
    "    results: List[Dict[str, str]] = []\n",
    "\n",
    "    for doc in documents:\n",
    "        doc_copy = doc.copy()\n",
    "        if content_field_name not in doc_copy:\n",
    "            continue\n",
    "        doc_content = doc_copy.pop(content_field_name) or \"\"\n",
    "\n",
    "        chunks = sliding_window(doc_content, size=size, step=step)\n",
    "        for ch in chunks:\n",
    "            if isinstance(ch['content'], str) and len(ch['content']) < min_chars:\n",
    "                continue\n",
    "            # merge metadata\n",
    "            ch.update(doc_copy)\n",
    "        results.extend([c for c in chunks if not isinstance(c['content'], str) or len(c['content']) >= min_chars])\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6010a505-b11e-457a-b284-c4212c40ae34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle(obj: Any, filename: str = \"docs.pkl\", compress: Optional[bool] = None) -> Path:\n",
    "    \"\"\"\n",
    "    Save any Python object to a pickle file in the current working directory.\n",
    "    If compress is None, auto-enable gzip when filename ends with '.gz'.\n",
    "    \"\"\"\n",
    "    path = Path.cwd() / filename\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    use_compress = (compress if compress is not None else filename.endswith(\".gz\"))\n",
    "    if use_compress:\n",
    "        with gzip.open(path, \"wb\") as f:\n",
    "            pickle.dump(obj, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    else:\n",
    "        with open(path, \"wb\") as f:\n",
    "            pickle.dump(obj, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    print(f\"Saved {type(obj).__name__} to {path}\")\n",
    "    return path\n",
    "\n",
    "def load_pickle(filename: str) -> Any:\n",
    "    \"\"\"\n",
    "    Load a pickle file from the current working directory.\n",
    "    WARNING: Only load pickles from sources you trust.\n",
    "    \"\"\"\n",
    "    path = Path.cwd() / filename\n",
    "    if filename.endswith(\".gz\"):\n",
    "        with gzip.open(path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    else:\n",
    "        with open(path, \"rb\") as f:\n",
    "            return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7c00642-b97e-4fe6-802c-1b06d3821c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved list to /workspaces/Agentic-AI-course/Project/arXiv_text.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/workspaces/Agentic-AI-course/Project/arXiv_text.pkl')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared_text = prepare_text_for_llm(raw_text)\n",
    "\n",
    "docs = [{\n",
    "        \"content\": prepared_text,\n",
    "        \"source\": \"arxiv:2005.11650\",\n",
    "        \"title\": \"Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting\",\n",
    "    }]\n",
    "\n",
    "# Pickle save \n",
    "save_pickle(docs, \"arXiv_text.pkl\")\n",
    "\n",
    "#docs = load_pickle(\"arXiv_text.pkl\")\n",
    "#docs[0]['content'] # text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "130ce900-2261-421a-b5d9-9511b73137ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75 chunks; first content preview:\n",
      "\n",
      "Title: Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks\n",
      "\n",
      "URL Source: https://ar5iv.org/html/2005.11650\n",
      "\n",
      "Markdown Content:\n",
      "(2020)\n",
      "\n",
      "###### Abstract.\n",
      "\n",
      "Modeling multivariate time series has long been a subject that has attracted researchers from a diverse range of fields including economics, finance, and traffic. A basic assumption behind multivariate time series forecasting is that its variables depend on one another but, upon looking closely, it’s fair to say that existing methods fail to fully exploit latent spatial dependencies between pairs of variables. In\n"
     ]
    }
   ],
   "source": [
    "# Typical LLM-friendly character windows (tune to your model/pipeline)\n",
    "chunks = chunk_documents(docs, size=2000, step=1000, content_field_name=\"content\", \n",
    "                         min_chars=200)\n",
    "\n",
    "# `chunks` is now a list of dicts ready for indexing/RAG\n",
    "print(f\"{len(chunks)} chunks; first content preview:\\n\")\n",
    "if chunks:\n",
    "    print(chunks[0][\"content\"][:600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ff556a-7692-47ef-b08c-8c0edcacd213",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94df34d-37cc-43b6-ae43-ee2323811c46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88725f2-c207-4619-a030-4639d3704c2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d6c2e7-ba65-45fc-9deb-de506ab8dd26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
