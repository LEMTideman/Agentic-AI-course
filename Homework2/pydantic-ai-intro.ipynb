{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c3119d8-aaff-46fd-95d4-4317852a5371",
   "metadata": {},
   "source": [
    "### Introduction to Pydantic AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ed2c133-f574-441f-8a3c-da5823fa0be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docs\n",
    "\n",
    "github_data = docs.read_github_data()\n",
    "parsed_data = docs.parse_data(github_data)\n",
    "chunks = docs.chunk_documents(parsed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b32df131-a729-47ef-ab2d-6551cc942fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x798499e5ea50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from minsearch import Index\n",
    "\n",
    "index = Index(\n",
    "    text_fields=[\"content\", \"filename\", \"title\", \"description\"],\n",
    ")\n",
    "\n",
    "index.fit(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a3ca40f-c002-405f-966f-e2e9d57d9a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, TypedDict\n",
    "\n",
    "class SearchResult(TypedDict):\n",
    "    \"\"\"Represents a single search result entry.\"\"\"\n",
    "    start: int\n",
    "    content: str\n",
    "    title: str\n",
    "    description: str\n",
    "    filename: str\n",
    "\n",
    "\n",
    "def search(query: str) -> List[SearchResult]:\n",
    "    \"\"\"\n",
    "    Search the index for documents matching the given query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query string.\n",
    "\n",
    "    Returns:\n",
    "        List[SearchResult]: A list of search results. Each result dictionary contains:\n",
    "            - start (int): The starting position or offset within the source file.\n",
    "            - content (str): A text excerpt or snippet containing the match.\n",
    "            - title (str): The title of the matched document.\n",
    "            - description (str): A short description of the document.\n",
    "            - filename (str): The path or name of the source file.\n",
    "    \"\"\"\n",
    "    return index.search(\n",
    "        query=query,\n",
    "        num_results=5,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "def264fd-f12c-431d-a154-1da6959e56c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_index = {}\n",
    "\n",
    "for item in parsed_data:\n",
    "    filename = item['filename']\n",
    "    content = item['content']\n",
    "    file_index[filename] = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2829d8e-f4c1-47a6-9d95-fffa02dbfd6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e6e68b1-e2a9-4289-a8c0-a7894d88deab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Retrieve the contents of a file from the file index if it exists.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The name of the file to read.\n",
    "\n",
    "    Returns:\n",
    "        str: The file's contents if found, otherwise an error message \n",
    "        indicating that the file does not exist.\n",
    "    \"\"\"\n",
    "    if filename in file_index:\n",
    "        return file_index[filename]\n",
    "    return \"File doesn't exist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c270492d-0964-4b4d-95e9-81578dd2c772",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59e088a4-0d6f-4fd1-8add-3327fb468611",
   "metadata": {},
   "outputs": [],
   "source": [
    "documentation_agent_instructions = \"\"\"\n",
    "You are an assistant that helps improve and generate high-quality documentation for the project.\n",
    "\n",
    "You have access to the following tools:\n",
    "- search — Use this to explore topics in depth. Make multiple search calls if needed to gather comprehensive information.\n",
    "- read_file — Use this when code snippets are missing or when you need to retrieve the full content of a file for context.\n",
    "\n",
    "If `read_file` cannot be used or the file content is unavailable, clearly state:\n",
    "> \"Unable to verify with read_file.\"\n",
    "\n",
    "When answering a question:\n",
    "1. Provide file references for all source materials.  \n",
    "   Use this format:  \n",
    "   [{filename}](https://github.com/evidentlyai/docs/blob/main/{filename})\n",
    "2. If the topic is covered in multiple documents, cite all relevant sources.\n",
    "3. Include code examples whenever they clarify or demonstrate the concept.\n",
    "4. Be concise, accurate, and helpful — focus on clarity and usability for developers.\n",
    "5. If documentation is missing or unclear, infer from context and note that explicitly.\n",
    "\n",
    "Example Citation:\n",
    "See the full implementation in [metrics/api_reference.md](https://github.com/evidentlyai/docs/blob/main/metrics/api_reference.md).\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ed13bcb-d2de-4fa0-9a0e-e2aca8e6572a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4be4fb19-d227-42f7-9b64-1bc09be4bbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "documentation_agent = Agent(\n",
    "    name='documentation_agent',\n",
    "    instructions=documentation_agent_instructions,\n",
    "    tools=[search, read_file],\n",
    "    model='openai:gpt-4o-mini'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc340e66-3348-42c7-a809-8bf3138ff8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toyaikit.chat import IPythonChatInterface\n",
    "from toyaikit.chat.runners import PydanticAIRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "725d165c-2b91-44dd-ab95-b3130e53eba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = PydanticAIRunner(\n",
    "    chat_interface=IPythonChatInterface(),\n",
    "    agent=documentation_agent\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24bf927-d040-4938-a721-6c9892f695f8",
   "metadata": {},
   "source": [
    "https://github.com/alexeygrigorev/toyaikit/blob/main/toyaikit/chat/runners.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5815956-7ad7-4210-83c8-260143db8c30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: How do I run LLM-as-a-judge evals? \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search(\"{\\\"query\\\":\\\"LLM-as-a-judge evals\\\"}\")</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>\"{\\\"query\\\":\\\"LLM-as-a-judge evals\\\"}\"</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>[{'start': 0, 'content': 'import CloudSignup from \\'/snippets/cloud_signup.mdx\\';\\nimport CreateProject from \\'/snippets/create_project.mdx\\';\\n\\nIn this tutorial, we\\'ll show how to evaluate text for custom criteria using LLM as the judge, and evaluate the LLM judge itself.\\n\\n<Info>\\n  **This is a local example.** You will run and explore results using the open-source Python library. At the end, we’ll optionally show how to upload results to the Evidently Platform for easy exploration.\\n</Info>\\n\\nWe\\'ll explore two ways to use an LLM as a judge:\\n\\n- **Reference-based**. Compare new responses against a reference. This is useful for regression testing or whenever you have a \"ground truth\" (approved responses) to compare against.\\n- **Open-ended**. Evaluate responses based on custom criteria, which helps evaluate new outputs when there\\'s no reference available.\\n\\nWe will focus on demonstrating **how to create and tune the LLM evaluator**, which you can then apply in different contexts, like regression testing or prompt comparison.\\n\\n<Info>\\n**Prefer videos?** We also have an extended code tutorial where we iteratively improve the prompt for LLM judge with a video walkthrough:  https://www.youtube.com/watch?v=kP_aaFnXLmY\\n</Info>\\n\\n## Tutorial scope\\n\\nHere\\'s what we\\'ll do:\\n\\n- **Create an evaluation dataset**. Create a toy Q&A dataset.\\n- **Create and run an LLM as a judge**. Design an LLM evaluator prompt.\\n- **Evaluate the judge**. Compare the LLM judge\\'s evaluations with manual labels.\\n\\nWe\\'ll start with the reference-based evaluator that determines whether a new response is correct (it\\'s more complex since it requires passing two columns to the prompt). Then, we\\'ll create a simpler judge focused on verbosity.\\n\\nTo complete the tutorial, you will need:\\n\\n- Basic Python knowledge.\\n- An OpenAI API key to use for the LLM evaluator.\\n\\nWe recommend running this tutorial in Jupyter Notebook or Google Colab to render rich HTML objects with summary results directly in a notebook cell.\\n\\n<Info>\\n  Run a sample notebook: [Jupyter ', 'title': 'LLM as a judge', 'description': 'How to create and evaluate an LLM judge.', 'filename': 'examples/LLM_judge.mdx'}, {'start': 1000, 'content': 'n.\\n\\n<Info>\\n**Prefer videos?** We also have an extended code tutorial where we iteratively improve the prompt for LLM judge with a video walkthrough:  https://www.youtube.com/watch?v=kP_aaFnXLmY\\n</Info>\\n\\n## Tutorial scope\\n\\nHere\\'s what we\\'ll do:\\n\\n- **Create an evaluation dataset**. Create a toy Q&A dataset.\\n- **Create and run an LLM as a judge**. Design an LLM evaluator prompt.\\n- **Evaluate the judge**. Compare the LLM judge\\'s evaluations with manual labels.\\n\\nWe\\'ll start with the reference-based evaluator that determines whether a new response is correct (it\\'s more complex since it requires passing two columns to the prompt). Then, we\\'ll create a simpler judge focused on verbosity.\\n\\nTo complete the tutorial, you will need:\\n\\n- Basic Python knowledge.\\n- An OpenAI API key to use for the LLM evaluator.\\n\\nWe recommend running this tutorial in Jupyter Notebook or Google Colab to render rich HTML objects with summary results directly in a notebook cell.\\n\\n<Info>\\n  Run a sample notebook: [Jupyter notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb) or [open it in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb).\\n</Info>\\n\\n## 1.  Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently\\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nimport numpy as np\\n\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\nfrom evidently import Report\\nfrom evidently import BinaryClassification\\nfrom evidently.descriptors import *\\nfrom evidently.presets import TextEvals, ValueStats, ClassificationPreset\\nfrom evidently.metrics import *\\n\\nfrom evidently.llm.templates import BinaryClassificationPromptTemplate\\n```\\n\\nPass your OpenAI key as an environment variable:\\n\\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\\n```\\n\\n<Info>\\n**Using other evaluator LLMs**. Check the [LLM judge docs](/metric', 'title': 'LLM as a judge', 'description': 'How to create and evaluate an LLM judge.', 'filename': 'examples/LLM_judge.mdx'}, {'start': 2000, 'content': 'notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb) or [open it in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb).\\n</Info>\\n\\n## 1.  Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently\\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nimport numpy as np\\n\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\nfrom evidently import Report\\nfrom evidently import BinaryClassification\\nfrom evidently.descriptors import *\\nfrom evidently.presets import TextEvals, ValueStats, ClassificationPreset\\nfrom evidently.metrics import *\\n\\nfrom evidently.llm.templates import BinaryClassificationPromptTemplate\\n```\\n\\nPass your OpenAI key as an environment variable:\\n\\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\\n```\\n\\n<Info>\\n**Using other evaluator LLMs**. Check the [LLM judge docs](/metrics/customize_llm_judge#change-the-evaluator-llm) to see how you can select a different evaluator LLM. \\n</Info>\\n\\n## 2.  Create the Dataset\\n\\nFirst, we\\'ll create a toy Q&A dataset with customer support question that includes:\\n\\n- **Questions**. The inputs sent to the LLM app.\\n- **Target responses**. The approved responses you consider accurate.\\n- **New responses**. Imitated new responses from the system.\\n- **Manual labels with explanation**. Labels that say if response is correct or not.\\n\\nWhy add the labels? It\\'s a good idea to be the judge yourself before you write a prompt. This helps:\\n\\n- Formulate better criteria. You discover nuances that help you write a better prompt.\\n- Get the \"ground truth\". You can use it to evaluate the quality of the LLM judge.\\n\\nUltimately, an LLM judge is a small ML system, and it needs its own evals\\\\!\\n\\n**Generate the dataframe**. Here\\'s how you can create this dataset in one go:\\n\\n<Accordion title=\"Toy data to run the example\" defaultOpen={false}>\\n  ```python\\n  ', 'title': 'LLM as a judge', 'description': 'How to create and evaluate an LLM judge.', 'filename': 'examples/LLM_judge.mdx'}, {'start': 20000, 'content': 'openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Verbosity\")\\n    ])\\n```\\n\\nRun the Report and view the summary results:\\xa0\\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(eval_dataset, None)\\nmy_eval\\n```\\n\\n![](/images/examples/llm_judge_tutorial_verbosity-min.png)\\n\\nYou can also view the dataframe using `eval_dataset.as_dataframe()`\\n\\n<Info>\\n  Don\\'t fully agree with the results? Use these labels as a starting point, edit the decisions where you see fit - now you\\'ve got your golden dataset\\\\! Next, iterate on your judge prompt. You can also try different evaluator LLMs to see which one does the job better. [How to change an LLM](/metrics/customize_llm_judge#change-the-evaluator-llm).\\n</Info>\\n\\n## What\\'s next?\\n\\nThe LLM judge itself is just one part of your overall evaluation framework. You can integrate this evaluator into different workflows, such as testing your LLM outputs after changing a prompt.\\n\\nTo be able to easily run and compare evals, systematically track the results, and interact with your evaluation dataset, you can use the Evidently Cloud platform.\\n\\n### Set up Evidently Cloud\\n\\n<CloudSignup />\\n\\nImport the components to connect with Evidently Cloud:\\n\\n```python\\nfrom evidently.ui.workspace import CloudWorkspace\\n```\\n\\n### Create a Project\\n\\n<CreateProject />\\n\\n### Send your eval\\n\\nSince you already created the eval, you can simply upload it to the Evidently Cloud.\\n\\n```python\\nws.add_run(project.id, my_eval, include_data=True)\\n```\\n\\nYou can then go to the Evidently Cloud, open your Project and explore the Report.\\n\\n![](/images/examples/llm_judge_tutorial_cloud-min.png)\\n\\n<Info>\\n  You can also [create the LLM judges with no-code](/docs/platform/evals_no_code).\\n</Info>\\n\\n# Reference documentation\\n\\nSee this page for complete [documentation on LLM judges](/metrics/customize_llm_judge).', 'title': 'LLM as a judge', 'description': 'How to create and evaluate an LLM judge.', 'filename': 'examples/LLM_judge.mdx'}, {'start': 17000, 'content': '\\nSince we already performed exact matching, you can see the crude accuracy of our judge. However, accuracy is not always the best metric. In this case, we might be more interested in recall: we want to make sure that the judge does not miss any \"incorrect\" answers .\\n\\n## 4. Evaluate the LLM Eval quality\\n\\nThis part is a bit meta: we\\'re going to evaluate the quality of our LLM evaluator itself\\\\! We can treat it as a simple **binary classification** problem.\\n\\n**Data definition**. To evaluate the classification quality, we need to map the structure of the dataset accordingly first. The column with the manual label is the \"target\", and the LLM-judge response is the \"prediction\":\\n\\n```python\\ndf=eval_dataset.as_dataframe()\\n\\ndefinition_2 = DataDefinition(\\n    classification=[BinaryClassification(\\n        target=\"label\",\\n        prediction_labels=\"Correctness\",\\n        pos_label = \"incorrect\")],\\n    categorical_columns=[\"label\", \"Correctness\"])\\n\\nclass_dataset = Dataset.from_pandas(\\n    pd.DataFrame(df),\\n    data_definition=definition_2)\\n```\\n\\n<Info>\\n  `Pos_label` refers to the class that is treated as the target (\"what we want to predict better\") for metrics like precision, recall, F1-score.\\n</Info>\\n\\n**Get a Report**. Let\\'s use a`ClassificationPreset()` that combines several classification metrics:\\n\\n```python\\nreport = Report([\\n    ClassificationPreset()\\n])\\n\\nmy_eval = report.run(class_dataset, None)\\nmy_eval\\n\\n# or my_eval.as_dict()\\n```\\n\\nWe can now get a well-rounded evaluation and explore the confusion matrix. We have one type of error each: overall the results are pretty good\\\\! You can also refine the prompt to try to improve them.\\n\\n![](/images/examples/llm_judge_tutorial_conf_matrix-min.png)\\n\\n## 5. Verbosity evaluator\\n\\nNext, let’s create a simpler verbosity judge. It will check whether the responses are concise and to the point. This only requires evaluating one output column: such checks are perfect for production evaluations where you don’t have a reference answer.\\n\\nHere\\'s ho', 'title': 'LLM as a judge', 'description': 'How to create and evaluate an LLM judge.', 'filename': 'examples/LLM_judge.mdx'}]</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>To run LLM-as-a-judge evaluations, you can follow a structured approach that includes creating an evaluation dataset, running the LLM as a judge, and evaluating the quality of the LLM's judgments. Here’s a concise guide based on the available resources:</p>\n",
       "<h3>1. Prerequisites</h3>\n",
       "<ul>\n",
       "<li><strong>Basic Python knowledge</strong>: Familiarity with Python coding is essential.</li>\n",
       "<li><strong>OpenAI API Key</strong>: Required to use the LLM evaluator.</li>\n",
       "</ul>\n",
       "<h3>2. Installation</h3>\n",
       "<p>First, install the Evidently library using pip:</p>\n",
       "<pre><code class=\"language-bash\">pip install evidently\n",
       "</code></pre>\n",
       "<h3>3. Import Required Modules</h3>\n",
       "<p>Here’s how to import the necessary modules to set up your evaluation:</p>\n",
       "<pre><code class=\"language-python\">import pandas as pd\n",
       "import numpy as np\n",
       "\n",
       "from evidently import Dataset\n",
       "from evidently import DataDefinition\n",
       "from evidently import Report\n",
       "from evidently import BinaryClassification\n",
       "from evidently.descriptors import *\n",
       "from evidently.presets import TextEvals, ValueStats, ClassificationPreset\n",
       "from evidently.metrics import *\n",
       "from evidently.llm.templates import BinaryClassificationPromptTemplate\n",
       "</code></pre>\n",
       "<h3>4. Setting Up API Key</h3>\n",
       "<p>Set your OpenAI API key as an environment variable:</p>\n",
       "<pre><code class=\"language-python\">import os\n",
       "os.environ[&quot;OPENAI_API_KEY&quot;] = &quot;YOUR_KEY&quot;\n",
       "</code></pre>\n",
       "<h3>5. Creating the Evaluation Dataset</h3>\n",
       "<p>You can create a toy Q&amp;A dataset that includes:</p>\n",
       "<ul>\n",
       "<li><strong>Questions</strong>: Inputs sent to the LLM.</li>\n",
       "<li><strong>Target responses</strong>: Approved, accurate responses.</li>\n",
       "<li><strong>New responses</strong>: Responses generated from the system.</li>\n",
       "<li><strong>Manual labels</strong>: Labels indicating if responses are correct.</li>\n",
       "</ul>\n",
       "<p>Here's an example of how to create this dataset:</p>\n",
       "<pre><code class=\"language-python\">data = {\n",
       "    &quot;question&quot;: [&quot;Question 1&quot;, &quot;Question 2&quot;],\n",
       "    &quot;target_response&quot;: [&quot;Correct answer 1&quot;, &quot;Correct answer 2&quot;],\n",
       "    &quot;new_response&quot;: [&quot;Generated answer 1&quot;, &quot;Generated answer 2&quot;],\n",
       "    &quot;manual_label&quot;: [&quot;correct&quot;, &quot;incorrect&quot;]\n",
       "}\n",
       "eval_dataset = pd.DataFrame(data)\n",
       "</code></pre>\n",
       "<h3>6. Running the LLM as a Judge</h3>\n",
       "<p>Design the LLM evaluator prompt and run it:</p>\n",
       "<pre><code class=\"language-python\">report = Report([TextEvals()])\n",
       "my_eval = report.run(eval_dataset, None)\n",
       "print(my_eval)\n",
       "</code></pre>\n",
       "<h3>7. Evaluating the LLM Judge's Quality</h3>\n",
       "<p>To evaluate the performance of your LLM judging:</p>\n",
       "<pre><code class=\"language-python\">df = eval_dataset.copy()\n",
       "df[&quot;Correctness&quot;] = [&quot;correct&quot;, &quot;incorrect&quot;]  # Example predictions from the LLM judge\n",
       "\n",
       "definition = DataDefinition(\n",
       "    classification=[\n",
       "        BinaryClassification(target=&quot;manual_label&quot;, prediction_labels=&quot;Correctness&quot;, pos_label=&quot;incorrect&quot;)\n",
       "    ],\n",
       "    categorical_columns=[&quot;manual_label&quot;, &quot;Correctness&quot;]\n",
       ")\n",
       "\n",
       "class_dataset = Dataset.from_pandas(df, data_definition=definition)\n",
       "\n",
       "# Generate a report on classification metrics\n",
       "report = Report([ClassificationPreset()])\n",
       "my_eval = report.run(class_dataset, None)\n",
       "print(my_eval)\n",
       "</code></pre>\n",
       "<h3>Further Considerations</h3>\n",
       "<ul>\n",
       "<li><strong>Verbosity Evaluator</strong>: You can customize the judge to assess verbosity or conciseness in responses.</li>\n",
       "<li><strong>Explore Different Evaluators</strong>: You can test various LLM evaluators to determine which one best suits your needs.</li>\n",
       "</ul>\n",
       "<h3>Resources</h3>\n",
       "<p>For detailed instructions, you can refer to the full implementation in the <a href=\"https://github.com/evidentlyai/docs/blob/main/examples/LLM_judge.mdx\">LLM as a judge tutorial</a>.</p>\n",
       "<p>With this setup, you should be able to effectively run and evaluate LLM-as-a-judge evaluations. If you have further questions or need more specific details, feel free to ask!</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: stop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat ended.\n"
     ]
    }
   ],
   "source": [
    "await runner.run();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f775320-7a71-4b1b-aa3e-ff9be2d0e095",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "519b98e6-cf37-49ce-94d7-de2a2e8d7fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = await documentation_agent.run(\n",
    "    user_prompt='how do I run llm as a judge evals',\n",
    "    # message_history=results.all_messages()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8fd2229c-c093-46fd-b69f-2d38fe9def7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request\n",
      "user-prompt\n",
      "\n",
      "response\n",
      "tool-call\n",
      "\n",
      "request\n",
      "tool-return\n",
      "\n",
      "response\n",
      "text\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for message in results.new_messages():\n",
    "    print(message.kind)\n",
    "\n",
    "    for part in message.parts:\n",
    "        print(part.part_kind)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9187e250-146f-4efa-be09-403722a77692",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260cf33c-6670-4791-895c-5324999a2f98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda25098-e5ef-4c6f-8424-5a0a32ec01e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
