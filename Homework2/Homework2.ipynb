{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86df502e-99d2-4c25-a937-fd6d17e47ec0",
   "metadata": {},
   "source": [
    "### Question 5: Build a Summary Agent Using Wikipedia Pages\n",
    "Given the Wikipedia page: https://en.wikipedia.org/wiki/Capybara, use the Fetch Web Page tool (fetch_url function) to get the content of the page and use the Save Summary tool to save the summary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f4763a5-f3e9-42ee-8623-0301f14b8cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent, function_tool, Runner # from OpenAI Agents SDK \n",
    "import requests\n",
    "from requests.exceptions import RequestException\n",
    "from typing import Any, Dict, List, Optional\n",
    "from pathlib import Path\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c875e34d-0b0e-42c6-8294-6e985f27274d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_url(url: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Fetch the textual content of a webpage.\n",
    "\n",
    "    Args:\n",
    "        url (str): The target URL to fetch content from.\n",
    "\n",
    "    Returns:\n",
    "        Optional[str]: The decoded HTML/text content of the fetched page if successful,\n",
    "        or None if an error occurred.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the provided URL is empty or invalid.\n",
    "    \"\"\"\n",
    "    if not url or not isinstance(url, str):\n",
    "        raise ValueError(\"The 'url' parameter must be a non-empty string.\")\n",
    "\n",
    "    jina_reader_base_url = \"https://r.jina.ai/\"\n",
    "    jina_reader_url = jina_reader_base_url + url.lstrip(\"/\")\n",
    "\n",
    "    try:\n",
    "        response = requests.get(jina_reader_url, timeout=10)\n",
    "        response.raise_for_status()  # Raises HTTPError for bad status codes\n",
    "        return response.content.decode(\"utf-8\")\n",
    "    except RequestException as e:\n",
    "        # Catch all network-related errors (e.g., ConnectionError, Timeout, HTTPError)\n",
    "        print(f\"Error fetching URL '{jina_reader_url}': {e}\")\n",
    "        return None\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"Error decoding response from '{jina_reader_url}'.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d0cc339-6bfe-4c2e-886a-f2f89c1d4bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_summary(filename: str, text: str, directory: str = \"summaries\") -> str:\n",
    "    \"\"\"\n",
    "    Save a summary to a UTF-8 .txt file inside a local 'summaries/' folder.\n",
    "\n",
    "    Args:\n",
    "        filename (str): Desired filename; '.txt' is appended if missing.\n",
    "        text (str): The summary to write.\n",
    "        directory (str): Target folder (created if needed).\n",
    "\n",
    "    Returns:\n",
    "        str: The absolute path to the saved file.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If filename or text is empty/invalid.\n",
    "    \"\"\"\n",
    "    if not filename or not isinstance(filename, str):\n",
    "        raise ValueError(\"filename must be a non-empty string.\")\n",
    "    if not text or not isinstance(text, str) or not text.strip():\n",
    "        raise ValueError(\"text must be a non-empty string.\")\n",
    "\n",
    "    # Basic sanitization: keep letters, numbers, dots, dashes, underscores\n",
    "    safe_name = re.sub(r\"[^A-Za-z0-9._-]\", \"_\", filename.strip())\n",
    "    if not safe_name.endswith(\".txt\"):\n",
    "        safe_name += \".txt\"\n",
    "\n",
    "    out_dir = Path(directory)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_path = (out_dir / safe_name).resolve()\n",
    "\n",
    "    # Write summary\n",
    "    out_path.write_text(text, encoding=\"utf-8\")\n",
    "    return str(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4b2abae-5539-4b3c-877d-210ade2b6fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test webpage retrieval function\n",
    "#content = fetch_url('https://en.wikipedia.org/wiki/Capybara/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "211c221e-e1b3-4870-b3b3-88f1149cbd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_agent = Agent(\n",
    "    name=\"web_agent\",\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=[function_tool(fetch_url), function_tool(save_summary)],\n",
    "    instructions=(\n",
    "        \"You are a helpful assistant that summarizes web pages. \"\n",
    "        \"When the user asks to summarize a URL, first call fetch_url(url=<the URL>) \"\n",
    "        \"to get the page text, then write a concise summary. \"\n",
    "        \"After you have the final summary text, call save_summary with \"\n",
    "        \"filename (end it with .txt) and text (the summary you just wrote). \"\n",
    "        \"In your final message to the user, include the saved file path and also show the summary.\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36e3c2c4-6eaa-41aa-a842-7fa5863ce961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunResult:\n",
      "- Last agent: Agent(name=\"web_agent\", ...)\n",
      "- Final output (str):\n",
      "    ### Summary of Capybara\n",
      "    \n",
      "    - The capybara (_Hydrochoerus hydrochaeris_) is the largest rodent in the world, native to South America.\n",
      "    - Known for its social behavior, capybaras often live in groups of 10-20, sometimes up to 100.\n",
      "    - They are semi-aquatic, inhabiting areas near rivers, lakes, and marshes, and are excellent swimmers.\n",
      "    - Capybaras primarily graze on grasses and aquatic plants, and practice coprophagy to aid their digestion.\n",
      "    - They are not considered endangered, but face threats from hunting and habitat destruction; they have also adapted well to urban environments.\n",
      "    \n",
      "    You can download the summary file [here](sandbox:/workspaces/Agentic-AI-course/Homework2/summaries/capybara_summary.txt).\n",
      "- 7 new item(s)\n",
      "- 4 raw response(s)\n",
      "- 0 input guardrail result(s)\n",
      "- 0 output guardrail result(s)\n",
      "(See `RunResult` for more details)\n"
     ]
    }
   ],
   "source": [
    "runner = Runner()\n",
    "\n",
    "# Ask the model to summarize *and* save:\n",
    "question = (\n",
    "    \"Summarize this page in ~5 bullet points and save it as 'capybara_summary.txt': \"\n",
    "    \"https://en.wikipedia.org/wiki/Capybara/\"\n",
    ")\n",
    "\n",
    "results = await runner.run(web_agent, input=question)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "184a333f-f9ed-4ba9-aa52-e864ebb5ae34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Summary of Capybara\n",
      "\n",
      "- The capybara (_Hydrochoerus hydrochaeris_) is the largest rodent in the world, native to South America.\n",
      "- Known for its social behavior, capybaras often live in groups of 10-20, sometimes up to 100.\n",
      "- They are semi-aquatic, inhabiting areas near rivers, lakes, and marshes, and are excellent swimmers.\n",
      "- Capybaras primarily graze on grasses and aquatic plants, and practice coprophagy to aid their digestion.\n",
      "- They are not considered endangered, but face threats from hunting and habitat destruction; they have also adapted well to urban environments.\n",
      "\n",
      "You can download the summary file [here](sandbox:/workspaces/Agentic-AI-course/Homework2/summaries/capybara_summary.txt).\n"
     ]
    }
   ],
   "source": [
    "# Inspect webpgage summary \n",
    "print(results.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8631f0b-f44f-4117-a786-563999df4876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect run items (for debugging purposes)\n",
    "items = results.new_items\n",
    "#print(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac337983-0ce3-49df-baca-46dd9fbec06b",
   "metadata": {},
   "source": [
    "### Question 6: Give the Agent a Search Tool \n",
    "Ask the agent to index multiple webpages. Use the agent's search tool to answer the following question: \"What are threats to capybara populations?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b748736d-a301-4713-b621-00339bd7f2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minsearch import AppendableIndex, VectorSearch\n",
    "from sentence_transformers import SentenceTransformer # necessary for semantic search\n",
    "from urllib.parse import urlparse, unquote\n",
    "from pathlib import PurePosixPath\n",
    "import numpy as np\n",
    "import time\n",
    "import docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "647da57e-71a7-448f-87dd-b84c274298b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://en.wikipedia.org/wiki/Lesser_capybara\",\n",
    "    \"https://en.wikipedia.org/wiki/Hydrochoerus\",\n",
    "    \"https://en.wikipedia.org/wiki/Neochoerus\", \n",
    "    \"https://en.wikipedia.org/wiki/Caviodon\", \n",
    "    \"https://en.wikipedia.org/wiki/Neochoerus_aesopi\"\n",
    "]\n",
    "\n",
    "def make_filename_from_url(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Use the last non-empty path segment of the URL as the base,\n",
    "    sanitize it, lowercase it, and append _summary.txt\n",
    "    \"\"\"\n",
    "    path = urlparse(url).path                 # e.g., '/wiki/Capybara'\n",
    "    parts = [p for p in PurePosixPath(path).parts if p not in (\"/\", \"\")]\n",
    "    base = parts[-1] if parts else \"page\"     # e.g., 'Capybara'\n",
    "    base = unquote(base) \n",
    "    base = re.sub(r\"[^A-Za-z0-9._-]+\", \"_\", base).strip(\"_\")\n",
    "    if not base:\n",
    "        base = \"page\"\n",
    "    return f\"{base.lower()}_summary.txt\"\n",
    "\n",
    "# Run serially\n",
    "for url in urls:\n",
    "    filename = make_filename_from_url(url)\n",
    "    question = f\"Summarize this page in ~5 bullet points and save it as '{filename}': {url}\"\n",
    "    results = await runner.run(web_agent, input=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e94bb31-8e62-4c32-b928-d2195f6864e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe71ad37-7e80-4d98-8ee1-1f36649b9982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_wiki_url_from_filename(filename: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    From 'lesser_capybara_summary.txt' -> 'https://en.wikipedia.org/wiki/Lesser_capybara'\n",
    "    From 'hydrochoerus_summary.txt'    -> 'https://en.wikipedia.org/wiki/Hydrochoerus'\n",
    "    Falls back to file:// path if pattern doesn't match.\n",
    "    \"\"\"\n",
    "    name = Path(filename).name\n",
    "    if not name.endswith(\"_summary.txt\"):\n",
    "        return None\n",
    "    base = name[: -len(\"_summary.txt\")]  # e.g., 'lesser_capybara'\n",
    "    if not base:\n",
    "        return None\n",
    "    # Capitalize first letter only, keep underscores as-is (matches your earlier file naming)\n",
    "    slug = base[0].upper() + base[1:]\n",
    "    return f\"https://en.wikipedia.org/wiki/{slug}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b54f3df-1611-4021-92f9-eaba40e858d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lexical_index_from_summaries(dir_path: str = \"./summaries\") -> AppendableIndex:\n",
    "    \"\"\"\n",
    "    Build an AppendableIndex over all .txt files in ./summaries.\n",
    "    Uses 'text' as the searchable field, and stores url/filename for filtering.\n",
    "    \"\"\"\n",
    "    index = AppendableIndex(\n",
    "        text_fields=[\"text\"],\n",
    "        keyword_fields=[\"url\", \"filename\"]\n",
    "    )\n",
    "\n",
    "    for p in Path(dir_path).glob(\"*.txt\"):\n",
    "        try:\n",
    "            content = p.read_text(encoding=\"utf-8\")\n",
    "            if not content.strip():\n",
    "                continue\n",
    "\n",
    "            url_guess = derive_wiki_url_from_filename(p.name)\n",
    "            doc = {\n",
    "                \"id\": f\"{p.stem}-{int(time.time())}\",  # unique id\n",
    "                \"text\": content,\n",
    "                \"url\": url_guess or f\"file://{p.resolve()}\",\n",
    "                \"filename\": p.name,\n",
    "                \"path\": str(p.resolve()),\n",
    "            }\n",
    "            index.append(doc)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipped {p}: {e}\")\n",
    "\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f031f229-0f84-4cfc-8e65-4653ffffd42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build once at import time; rebuild this if you add more files later.\n",
    "LEXICAL_INDEX = build_lexical_index_from_summaries(\"./summaries\")\n",
    "\n",
    "# Keep a simple log of queries for your assignment submission\n",
    "SEARCH_LOG: List[Dict[str, Any]] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8967604-e0fd-4bd9-88a1-1763cd2a0198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'query': 'threats to capybara populations',\n",
       "  'filter_url': None,\n",
       "  'num_results': 5,\n",
       "  'ts': 1760955641.6864622},\n",
       " {'query': 'threats to capybara populations',\n",
       "  'filter_url': None,\n",
       "  'num_results': 5,\n",
       "  'ts': 1760956052.2308307},\n",
       " {'query': 'threats to capybara populations',\n",
       "  'filter_url': None,\n",
       "  'num_results': 5,\n",
       "  'ts': 1760956092.121735}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEARCH_LOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "539d2148-d079-4093-95e4-2871912204a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_corpus(query: str, filter_url: Optional[str] = None, num_results: int = 5):\n",
    "    \"\"\"\n",
    "    Simple lexical search over ./summaries using minsearch.AppendableIndex.\n",
    "    Always call this tool before answering questions about the summaries.\n",
    "    \"\"\"\n",
    "    if not isinstance(query, str) or not query.strip():\n",
    "        raise ValueError(\"query must be a non-empty string.\")\n",
    "\n",
    "    # record for your submission\n",
    "    SEARCH_LOG.append({\n",
    "        \"query\": query,\n",
    "        \"filter_url\": filter_url,\n",
    "        \"num_results\": num_results,\n",
    "        \"ts\": time.time(),\n",
    "    })\n",
    "\n",
    "    filter_dict = {\"url\": filter_url} if filter_url else None\n",
    "    results = LEXICAL_INDEX.search(query, filter_dict=filter_dict, num_results=num_results)\n",
    "    return results  # list of dicts: each includes 'text', 'url', 'filename', etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b71b187d-95f4-468c-a204-74e36ae13577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capybara populations face several threats, including:\n",
      "\n",
      "1. **Hunting**: Capybaras are hunted for their meat and skin, impacting their numbers.\n",
      "2. **Habitat Destruction**: Their natural habitats, primarily near rivers, lakes, and marshes, are increasingly being destroyed for agricultural development and urban expansion.\n",
      "3. **Urban Adaptation**: While they adapt well to urban areas, this can lead to conflicts with humans, further threatening their populations.\n",
      "\n",
      "Despite these challenges, capybaras are not currently considered endangered. \n",
      "\n",
      "**Source:** The summaries highlight the threats as hunting and habitat destruction, indicating that although capybaras thrive in some environments, they still face significant risks.\n",
      "\n",
      "**Search queries used:**\n",
      "- threats to capybara populations\n"
     ]
    }
   ],
   "source": [
    "# Specialized search Agent (one task only, simple instructions)\n",
    "search_agent = Agent(\n",
    "    name=\"search_agent\",\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=[function_tool(search_corpus)],\n",
    "    instructions=(\n",
    "        \"You answer questions ONLY by searching the local summaries corpus via the `search_corpus` tool.\\n\"\n",
    "        \"Process:\\n\"\n",
    "        \"1) Break the user's question into 1–3 concrete search queries.\\n\"\n",
    "        \"2) Call `search_corpus` for each query (optionally pass filter_url to narrow to a page).\\n\"\n",
    "        \"3) Read the returned snippets and synthesize a concise answer grounded in them.\\n\"\n",
    "        \"Rules:\\n\"\n",
    "        \"- Do not rely on prior knowledge; if nothing is found, say so and suggest indexing more docs.\\n\"\n",
    "        \"- When helpful, mention the source filenames/URLs from the results.\\n\"\n",
    "        \"- In your final message, include a 'Search queries used:' list showing the exact queries you issued.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "runner = Runner()\n",
    "\n",
    "# Your assignment question:\n",
    "question = \"What are threats to capybara populations?\"\n",
    "results = await runner.run(search_agent, input=question)\n",
    "\n",
    "print(results.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13436532-df98-43bd-8959-b295ee525c4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0e70312-e227-4526-ab90-cb781c57d132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capybara populations face several threats, including:\n",
      "\n",
      "- **Hunting:** Capybaras are often hunted for their meat and hides, which can significantly reduce local populations.\n",
      "- **Habitat Destruction:** Urbanization and agricultural expansion lead to fragmentation and loss of their natural habitats, particularly near rivers and lakes.\n",
      "- **Human-Wildlife Conflict:** As humans expand into capybara territories, conflicts can arise, leading to further pressures on these animals.\n",
      "- **Climate Change:** Changes in climate can affect their aquatic habitats and the availability of food sources.\n",
      "- **Predation:** While adults have few natural predators, young capybaras are vulnerable to birds of prey, large mammals, and reptiles.\n",
      "\n",
      "Despite these threats, capybaras are not currently considered endangered and have shown adaptability to urban environments.\n",
      "\n",
      "**Search queries used:**\n",
      "- threats to capybara populations\n"
     ]
    }
   ],
   "source": [
    "# Generic Agent (choice between two tasks, complex instructions)\n",
    "multi_task_agent = Agent(\n",
    "    name=\"web_agent\",\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=[\n",
    "        function_tool(fetch_url),\n",
    "        function_tool(save_summary),\n",
    "        function_tool(search_corpus),\n",
    "    ],\n",
    "    instructions=(\n",
    "        \"ROLE: You can do two tasks:\\n\"\n",
    "        \"A) Summarize a web page from a URL and (optionally) save it to disk.\\n\"\n",
    "        \"B) Answer questions using the local summaries corpus via the `search_corpus` tool.\\n\"\n",
    "        \"\\n\"\n",
    "        \"TOOL POLICY:\\n\"\n",
    "        \"- If the user's message contains a URL or asks to summarize: \"\n",
    "        \"  1) Call fetch_url(url=<the URL>), 2) write a concise summary, \"\n",
    "        \"  3) If the user asked you to save (or gave a filename), call save_summary(filename=<name>, text=<summary>). \"\n",
    "        \"  4) In your final message, include the saved file path (if saved) and show the summary.\\n\"\n",
    "        \"- If the user's message is a knowledge question without a URL: \"\n",
    "        \"  1) Break the question into 1–3 concrete search queries, \"\n",
    "        \"  2) Call search_corpus for each query (optionally use filter_url to narrow), \"\n",
    "        \"  3) Synthesize an answer strictly from the returned hits. Do not rely on prior knowledge.\\n\"\n",
    "        \"- If a single message asks for both (e.g., summarize *and* then answer a follow-up): \"\n",
    "        \"  do the summarize flow first, then run the search flow.\\n\"\n",
    "        \"- Do not call save_summary during search mode unless the user explicitly asks to save your answer.\\n\"\n",
    "        \"\\n\"\n",
    "        \"OUTPUT FORMAT:\\n\"\n",
    "        \"- Summarize mode: 3–6 bullet points + one-sentence takeaway. If saved, add: 'Saved to: <path>'.\\n\"\n",
    "        \"- Search mode: 3–6 bullet points grounded in retrieved snippets. \"\n",
    "        \"  When useful, cite sources with [filename] or the URL slug. \"\n",
    "        \"  End with a short 'Search queries used:' list showing the exact queries you sent to search_corpus.\\n\"\n",
    "        \"\\n\"\n",
    "        \"TOOL SIGNATURES (for your reference):\\n\"\n",
    "        \"- fetch_url(url: str) -> str\\n\"\n",
    "        \"- save_summary(filename: str, text: str, directory?: str='summaries') -> str\\n\"\n",
    "        \"- search_corpus(query: str, filter_url?: str, num_results?: int=5) -> List[Result]\\n\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "runner = Runner()\n",
    "\n",
    "# Your assignment question:\n",
    "question = \"What are threats to capybara populations?\"\n",
    "results = await runner.run(multi_task_agent, input=question)\n",
    "\n",
    "print(results.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4675341f-ae54-4c7d-800a-de7ab478c22e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f658b21-d117-4362-8b14-629ddac891fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ca261d-7003-4a74-a065-2eb4f0560b75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04575786-4cb9-4b7e-b477-6dd2ea677140",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
